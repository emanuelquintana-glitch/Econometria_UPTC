\documentclass[12pt,a4paper]{article}

%=============================================================================
%  PAQUETES
%=============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\selectlanguage{spanish}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{hyperref}

\geometry{
  a4paper,
  top=2.5cm, bottom=2.5cm,
  left=2.8cm, right=2.8cm,
  headheight=15pt
}

%=============================================================================
%  COLORES INSTITUCIONALES UPTC
%=============================================================================
\definecolor{uptcBlue}{RGB}{0,56,116}
\definecolor{uptcGold}{RGB}{182,145,3}
\definecolor{lightBlue}{RGB}{230,240,255}
\definecolor{lightGold}{RGB}{255,250,220}
\definecolor{alertRed}{RGB}{180,0,0}
\definecolor{darkGray}{RGB}{60,60,60}

%=============================================================================
%  ENCABEZADO Y PIE DE PÁGINA
%=============================================================================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\color{uptcBlue}\textbf{Econometría --- UPTC}}
\fancyhead[R]{\small\color{uptcBlue}Problema 10.1: Multicolinealidad Perfecta}
\fancyfoot[C]{\small\color{darkGray}\thepage}
\renewcommand{\headrulewidth}{0.6pt}
\renewcommand{\footrulewidth}{0.3pt}

%=============================================================================
%  TÍTULOS DE SECCIÓN
%=============================================================================
\titleformat{\section}{\large\bfseries\color{uptcBlue}}{{\thesection.}}{0.5em}{}[\color{uptcGold}\titlerule]
\titleformat{\subsection}{\normalsize\bfseries\color{uptcBlue}}{{\thesubsection.}}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\color{darkGray}}{{\thesubsubsection.}}{0.5em}{}

%=============================================================================
%  CAJAS DESTACADAS
%=============================================================================
\tcbuselibrary{breakable, skins, theorems}

\newtcolorbox{definicion}[1][]{
  colback=lightBlue, colframe=uptcBlue,
  fonttitle=\bfseries\small, title=Definición,
  breakable, #1
}
\newtcolorbox{teorema}[1][]{
  colback=lightGold, colframe=uptcGold,
  fonttitle=\bfseries\small, title=Teorema,
  breakable, #1
}
\newtcolorbox{resultado}[1][]{
  colback=lightBlue!60, colframe=uptcBlue!80,
  fonttitle=\bfseries\small,
  breakable, #1
}
\newtcolorbox{alerta}[1][]{
  colback=red!8, colframe=alertRed,
  fonttitle=\bfseries\small, title=¡Resultado Crítico!,
  breakable, #1
}
\newtcolorbox{corolario}[1][]{
  colback=green!7, colframe=green!50!black,
  fonttitle=\bfseries\small, title=Corolario,
  breakable, #1
}

%=============================================================================
%  AMBIENTES MATEMÁTICOS
%=============================================================================
\newtheorem{prop}{Proposición}[section]
\newtheorem{lema}[prop]{Lema}
\newtheorem*{dem}{Demostración}

%=============================================================================
%  OPERADORES
%=============================================================================
\DeclareMathOperator{\rg}{rg}
\DeclareMathOperator{\tr}{tr}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\E}{\operatorname{E}}

%=============================================================================
%  NOTACIÓN COMPACTA PARA SUMATORIAS FRECUENTES
%=============================================================================
\newcommand{\sxdos}{\displaystyle\sum_{i=1}^{n} x_{2i}^{2}}
\newcommand{\sxtres}{\displaystyle\sum_{i=1}^{n} x_{3i}^{2}}
\newcommand{\sxdxt}{\displaystyle\sum_{i=1}^{n} x_{2i}x_{3i}}
\newcommand{\sxdy}{\displaystyle\sum_{i=1}^{n} x_{2i}y_{i}}
\newcommand{\sxty}{\displaystyle\sum_{i=1}^{n} x_{3i}y_{i}}

%=============================================================================
%  DOCUMENTO
%=============================================================================
\begin{document}

%-----------------------------------------------------------------------------
%  PORTADA
%-----------------------------------------------------------------------------
\begin{titlepage}
  \begin{center}
    \vspace*{1cm}
    {\color{uptcBlue}\rule{\linewidth}{2pt}}\\[0.4cm]
    {\color{uptcBlue}\rule{\linewidth}{0.8pt}}\\[1.2cm]

    {\Huge\bfseries\color{uptcBlue}
      Problema 10.1\\[0.3cm]
      Multicolinealidad Perfecta\\[0.3cm]
      en el Modelo de Regresión\\[0.2cm]
      Lineal Múltiple}\\[1.4cm]

    {\color{uptcGold}\rule{8cm}{1.5pt}}\\[0.8cm]

    {\large\bfseries\color{darkGray}
      Demostración de la Imposibilidad de Estimación\\[0.2cm]
      de los Coeficientes MCO bajo Dependencia Lineal Perfecta}\\[1.8cm]

    \begin{minipage}{0.65\textwidth}
      \centering
      \begin{tabular}{rl}
        \textcolor{uptcBlue}{\textbf{Estudiante:}} & Emanuel Quintana Silva \\[4pt]
        \textcolor{uptcBlue}{\textbf{Asignatura:}} & Econometría \\[4pt]
        \textcolor{uptcBlue}{\textbf{Universidad:}} & Universidad Pedagógica y \\
                                                    & Tecnológica de Colombia (UPTC) \\[4pt]
        \textcolor{uptcBlue}{\textbf{Referencia:}} & Gujarati \& Porter, Cap.~10 \\[4pt]
        \textcolor{uptcBlue}{\textbf{Fecha:}} & \today
      \end{tabular}
    \end{minipage}\\[2cm]

    {\color{uptcBlue}\rule{\linewidth}{0.8pt}}\\[0.2cm]
    {\color{uptcBlue}\rule{\linewidth}{2pt}}
  \end{center}
\end{titlepage}

%-----------------------------------------------------------------------------
%  TABLA DE CONTENIDO
%-----------------------------------------------------------------------------
\tableofcontents
\newpage

%=============================================================================
\section{Enunciado Formal del Problema}
%=============================================================================

\begin{resultado}[title=Enunciado~10.1 (Gujarati \& Porter)]
En el modelo de regresión lineal de $k$ variables, hay $k$ ecuaciones normales para
estimar las $k$ incógnitas.  Suponga que $X_k$ es una combinación lineal perfecta de
las variables $X$ restantes.  ¿Cómo se demostraría que en este caso es \textbf{imposible}
estimar los $k$ coeficientes de regresión?\\[4pt]
Se dispone del siguiente conjunto de datos ($n=11$ observaciones):
\end{resultado}

\vspace{0.5em}
\begin{center}
\begin{tabular}{c >{\columncolor{lightBlue!60}}r rr}
  \toprule
  \rowcolor{uptcBlue!15}
  $i$ & $Y_i$ & $X_{2i}$ & $X_{3i}$ \\
  \midrule
   1 & $-10$ &  1 &  1 \\
   2 &  $-8$ &  2 &  3 \\
   3 &  $-6$ &  3 &  5 \\
   4 &  $-4$ &  4 &  7 \\
   5 &  $-2$ &  5 &  9 \\
   6 &   $0$ &  6 & 11 \\
   7 &   $2$ &  7 & 13 \\
   8 &   $4$ &  8 & 15 \\
   9 &   $6$ &  9 & 17 \\
  10 &   $8$ & 10 & 19 \\
  11 &  $10$ & 11 & 21 \\
  \midrule
  \rowcolor{uptcGold!20}
  $\boldsymbol{\Sigma}$ & $\mathbf{0}$ & $\mathbf{66}$ & $\mathbf{121}$ \\
  \bottomrule
\end{tabular}
\end{center}

%=============================================================================
\section{Marco Teórico: El Estimador MCO en Forma Matricial}
%=============================================================================

\subsection{El Modelo de Regresión Lineal Múltiple}

El modelo poblacional de regresión lineal con $k-1$ regresores (incluyendo el
intercepto $X_1 \equiv 1$) es:

\begin{equation}
  Y_i = \beta_1 + \beta_2 X_{2i} + \beta_3 X_{3i} + \cdots + \beta_k X_{ki} + u_i,
  \qquad i = 1, 2, \ldots, n
\end{equation}

En notación matricial compacta:

\begin{equation}
  \bm{Y} = \bm{X\beta} + \bm{u}
\end{equation}

donde $\bm{Y}$ es el vector $(n\times1)$ de observaciones,
$\bm{X}$ es la \textbf{matriz de diseño} $(n\times k)$,
$\bm{\beta}$ es el vector $(k\times1)$ de parámetros desconocidos, y
$\bm{u}$ es el vector $(n\times1)$ de perturbaciones estocásticas.

\begin{definicion}
  Bajo los \textbf{supuestos de Gauss-Markov} ($\E[\bm{u}|\bm{X}]=\bm{0}$,
  $\Var[\bm{u}|\bm{X}]=\sigma^2\bm{I}_n$, $\rg(\bm{X})=k$), el estimador MCO se obtiene
  minimizando la Suma de Residuos al Cuadrado:
  \[
    \text{SRC}(\bm{\beta}) \;=\; \bm{u}'\bm{u}
    \;=\; (\bm{Y}-\bm{X\beta})'(\bm{Y}-\bm{X\beta})
  \]
  cuya solución única es el \textbf{Estimador de Mínimos Cuadrados Ordinarios}:
  \begin{equation}
    \hat{\bm{\beta}} \;=\; (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}
    \label{eq:mco}
  \end{equation}
\end{definicion}

El supuesto $\rg(\bm{X})=k$ (rango completo de columnas) es \emph{indispensable}
para que la inversa $(\bm{X}'\bm{X})^{-1}$ exista.

\subsection{Las \texorpdfstring{$k$}{k} Ecuaciones Normales}

La condición de primer orden $\partial\,\text{SRC}/\partial\bm{\beta}=\bm{0}$
produce el sistema de $k$ \textbf{ecuaciones normales}:

\begin{equation}
  \bm{X}'\bm{X}\,\hat{\bm{\beta}} \;=\; \bm{X}'\bm{Y}
  \label{eq:normales}
\end{equation}

Para el modelo en \textbf{desviaciones respecto a la media}
($x_{ji} = X_{ji}-\bar{X}_j$, $y_i = Y_i - \bar{Y}$), el intercepto se elimina y el
sistema \eqref{eq:normales} se reduce a las $k-1$ ecuaciones:

\begin{equation}
  \begin{pmatrix}
    \displaystyle\sum x_{2i}^2 & \displaystyle\sum x_{2i}x_{3i} \\[8pt]
    \displaystyle\sum x_{2i}x_{3i} & \displaystyle\sum x_{3i}^2
  \end{pmatrix}
  \begin{pmatrix} \hat{\beta}_2 \\[8pt] \hat{\beta}_3 \end{pmatrix}
  =
  \begin{pmatrix}
    \displaystyle\sum x_{2i}y_i \\[8pt]
    \displaystyle\sum x_{3i}y_i
  \end{pmatrix}
  \label{eq:sistema2}
\end{equation}

%=============================================================================
\section{Identificación de la Combinación Lineal Perfecta}
%=============================================================================

\subsection{Detección Empírica}

Examinando columna a columna el conjunto de datos:

\begin{center}
\begin{tabular}{crrr}
  \toprule
  \rowcolor{uptcBlue!15}
  $i$ & $X_{2i}$ & $2X_{2i}-1$ & $X_{3i}$ \\
  \midrule
  1  &  1 &  1 &  1 \\
  2  &  2 &  3 &  3 \\
  3  &  3 &  5 &  5 \\
  4  &  4 &  7 &  7 \\
  5  &  5 &  9 &  9 \\
  6  &  6 & 11 & 11 \\
  7  &  7 & 13 & 13 \\
  8  &  8 & 15 & 15 \\
  9  &  9 & 17 & 17 \\
  10 & 10 & 19 & 19 \\
  11 & 11 & 21 & 21 \\
  \bottomrule
\end{tabular}
\end{center}

\vspace{0.4em}
\begin{alerta}
La tabla anterior demuestra que, \textbf{sin excepción en ninguna de las 11 observaciones}:
\[
  X_{3i} \;=\; 2X_{2i} - 1, \qquad \forall\, i = 1,\ldots,11
\]
Puesto que la columna de intercepto es $X_1 \equiv 1$, la relación puede escribirse
como combinación lineal de las columnas de $\bm{X}$:
\[
  \boxed{-1\cdot X_{1i} \;-\; 2\cdot X_{2i} \;+\; 1\cdot X_{3i} \;=\; 0,
  \qquad \forall\,i}
\]
Los coeficientes $(\lambda_1,\lambda_2,\lambda_3)=(-1,-2,1)$, no todos nulos, son los
que atestiguan la dependencia lineal perfecta.
\end{alerta}

\subsection{Reformulación en Forma de Desviaciones}

Calculamos las medias:
\[
  \bar{Y} = \frac{0}{11} = 0, \qquad
  \bar{X}_2 = \frac{66}{11} = 6, \qquad
  \bar{X}_3 = \frac{121}{11} = 11
\]

Las variables en desviaciones son:
$x_{2i} = X_{2i}-6$ y $x_{3i} = X_{3i}-11$.

Como $X_{3i} = 2X_{2i}-1$, restando la media de $X_3$:
\[
  x_{3i} = X_{3i}-11 = (2X_{2i}-1)-11 = 2X_{2i}-12 = 2(X_{2i}-6) = 2x_{2i}
\]

\begin{resultado}[title=Relación de dependencia en desviaciones]
\[
  x_{3i} \;=\; 2\,x_{2i}, \qquad \forall\,i=1,\ldots,11
\]
La constante de proporcionalidad es $\lambda = 2$.  En el espacio de desviaciones,
la multicolinealidad perfecta se manifiesta como \textbf{proporcionalidad exacta}.
\end{resultado}

\subsection{Cálculo Explícito de los Momentos Muestrales}

La tabla siguiente compila los productos cruzados que conforman la matriz $\bm{X}'\bm{X}$
y el vector $\bm{X}'\bm{Y}$ en desviaciones:

\vspace{0.4em}
\begin{center}
\small
\begin{tabular}{c rrrr rrr rrr}
  \toprule
  \rowcolor{uptcBlue!15}
  $i$ & $y_i$ & $x_{2i}$ & $x_{3i}$
      & $x_{2i}^2$ & $x_{3i}^2$ & $x_{2i}x_{3i}$
      & $x_{2i}y_i$ & $x_{3i}y_i$ \\
  \midrule
   1 & $-10$ & $-5$ & $-10$ & 25  & 100 &  50 &  50 & 100 \\
   2 &  $-8$ & $-4$ &  $-8$ & 16  &  64 &  32 &  32 &  64 \\
   3 &  $-6$ & $-3$ &  $-6$ &  9  &  36 &  18 &  18 &  36 \\
   4 &  $-4$ & $-2$ &  $-4$ &  4  &  16 &   8 &   8 &  16 \\
   5 &  $-2$ & $-1$ &  $-2$ &  1  &   4 &   2 &   2 &   4 \\
   6 &   $0$ &  $0$ &   $0$ &  0  &   0 &   0 &   0 &   0 \\
   7 &   $2$ &  $1$ &   $2$ &  1  &   4 &   2 &   2 &   4 \\
   8 &   $4$ &  $2$ &   $4$ &  4  &  16 &   8 &   8 &  16 \\
   9 &   $6$ &  $3$ &   $6$ &  9  &  36 &  18 &  18 &  36 \\
  10 &   $8$ &  $4$ &   $8$ & 16  &  64 &  32 &  32 &  64 \\
  11 &  $10$ &  $5$ &  $10$ & 25  & 100 &  50 &  50 & 100 \\
  \midrule
  \rowcolor{uptcGold!25}
  $\Sigma$ & $0$ & $0$ & $0$
           & $\mathbf{110}$ & $\mathbf{440}$ & $\mathbf{220}$
           & $\mathbf{220}$ & $\mathbf{440}$ \\
  \bottomrule
\end{tabular}
\end{center}

\vspace{0.4em}
\noindent En consecuencia, la matriz de momentos y el vector de productos cruzados son:

\begin{equation}
  \bm{M} \;=\; \bm{X}'\bm{X}
  \;=\;
  \begin{pmatrix} 110 & 220 \\ 220 & 440 \end{pmatrix},
  \qquad
  \bm{X}'\bm{Y}
  \;=\;
  \begin{pmatrix} 220 \\ 440 \end{pmatrix}
  \label{eq:momentos}
\end{equation}

%=============================================================================
\section{Demostración General: Imposibilidad de Estimación}
%=============================================================================

\subsection{Proposición Central}

\begin{prop}[Indeterminación bajo Multicolinealidad Perfecta]
  Sea $\bm{X}$ la matriz de diseño $(n\times k)$.  Si existe un vector no nulo
  $\bm{c}\in\mathbb{R}^k$ tal que $\bm{X}\bm{c}=\bm{0}$, entonces el sistema de
  ecuaciones normales \eqref{eq:normales} \textbf{no tiene solución única} y el
  estimador MCO $\hat{\bm{\beta}}=(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}$ \textbf{no existe}.
\end{prop}

\begin{dem}
Procederemos demostrando que $\bm{X}'\bm{X}$ es singular, lo que impide su inversión.

\medskip
\noindent\textbf{Paso 1. De la dependencia en $\bm{X}$ a la singularidad de
$\bm{X}'\bm{X}$.}

Si $\bm{Xc}=\bm{0}$ con $\bm{c}\neq\bm{0}$, premultiplicando por $\bm{X}'$:
\[
  \bm{X}'(\bm{Xc}) = \bm{X}'\bm{0} = \bm{0}
  \;\Longrightarrow\;
  (\bm{X}'\bm{X})\bm{c} = \bm{0}
\]
Por definición, $\bm{c}\neq\bm{0}$ es un \textbf{vector en el núcleo} de
$\bm{X}'\bm{X}$.  Una matriz tiene núcleo no trivial si y solo si es
\textbf{singular}, es decir:
\[
  \det(\bm{X}'\bm{X}) = 0
\]

\medskip
\noindent\textbf{Paso 2. La inversa no existe.}

La inversa de una matriz cuadrada existe si y solo si su determinante es
distinto de cero.  Como $\det(\bm{X}'\bm{X})=0$, la expresión
$(\bm{X}'\bm{X})^{-1}$ \textbf{no está definida} y, por tanto,
$\hat{\bm{\beta}}=(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}$ tampoco.

\medskip
\noindent\textbf{Paso 3. El sistema tiene infinitas soluciones.}

Sea $\hat{\bm{\beta}}^*$ cualquier solución particular del sistema
$\bm{X}'\bm{X}\hat{\bm{\beta}}=\bm{X}'\bm{Y}$.  Para todo escalar
$\alpha\in\mathbb{R}$, el vector:
\[
  \hat{\bm{\beta}}^* + \alpha\bm{c}
\]
también es solución, pues:
\[
  \bm{X}'\bm{X}(\hat{\bm{\beta}}^* + \alpha\bm{c})
  = \bm{X}'\bm{X}\hat{\bm{\beta}}^* + \alpha\underbrace{(\bm{X}'\bm{X})\bm{c}}_{=\bm{0}}
  = \bm{X}'\bm{Y}
\]
Existe una \textbf{familia de infinitas soluciones} parametrizada por $\alpha$.
No hay forma algebraica de elegir una solución única: la minimización de la
SRC no identifica un único $\hat{\bm{\beta}}$.
\hfill$\square$
\end{dem}

\subsection{Demostración Alternativa: Rango y el Teorema de la Dimensión}

\begin{teorema}[title={Rango de $\bm{X}'\bm{X}$}]
  Para toda matriz $\bm{X}$, se cumple $\rg(\bm{X}'\bm{X}) = \rg(\bm{X})$.
\end{teorema}

Si las columnas de $\bm{X}$ son linealmente dependientes,
$\rg(\bm{X}) < k$.  Entonces:
\[
  \rg(\bm{X}'\bm{X}) = \rg(\bm{X}) < k
\]
Una matriz cuadrada $k\times k$ con rango menor que $k$ tiene determinante nulo
y no es invertible.

\subsection{Demostración vía el Determinante: Desigualdad de Cauchy-Schwarz}

El determinante de la matriz $2\times2$ de momentos en desviaciones es:

\begin{equation}
  D \;=\; \det(\bm{M})
  \;=\; \left(\sum x_{2i}^2\right)\!\left(\sum x_{3i}^2\right)
        - \left(\sum x_{2i}x_{3i}\right)^2
\end{equation}

La \textbf{desigualdad de Cauchy-Schwarz} para sumas finitas establece:
\[
  \left(\sum x_{2i}x_{3i}\right)^2
  \;\leq\;
  \left(\sum x_{2i}^2\right)\!\left(\sum x_{3i}^2\right)
\]
con igualdad \textbf{si y solo si} existe $\lambda\in\mathbb{R}$ tal que
$x_{3i}=\lambda\,x_{2i}$ para todo $i$, es decir, si y solo si hay
\emph{proporcionalidad exacta} (multicolinealidad perfecta en desviaciones).

\begin{alerta}
Cuando la igualdad de Cauchy-Schwarz se alcanza:
\[
  D = \left(\sum x_{2i}^2\right)\!\left(\sum x_{3i}^2\right)
      - \left(\sum x_{2i}x_{3i}\right)^2 = 0
\]
La matriz $\bm{M}$ es singular: \textbf{el sistema de ecuaciones normales no tiene
solución única y los estimadores MCO son indeterminados.}
\end{alerta}

%=============================================================================
\section{Verificación Numérica con los Datos del Problema 10.1}
%=============================================================================

\subsection{La Relación de Dependencia Lineal: Verificación Algebraica}

La relación $X_{3i}=2X_{2i}-1$ implica, en desviaciones:

\begin{align}
  x_{3i} &= X_{3i} - \bar{X}_3
           = (2X_{2i}-1) - (2\bar{X}_2-1)
           = 2(X_{2i}-\bar{X}_2)
           = 2x_{2i}
\end{align}

Por tanto, $\lambda=2$, y el vector de dependencia sobre $\bm{X}$
(incluyendo el intercepto) es:
\[
  \bm{Xc} = -1\cdot\bm{1} - 2\cdot\bm{X}_2 + 1\cdot\bm{X}_3 = \bm{0}
\]

\subsection{Cálculo Explícito del Determinante}

Sustituyendo los valores de la Tabla~1 en la expresión del determinante:

\begin{align}
  D &= \left(\sxdos\right)\!\left(\sxtres\right) - \left(\sxdxt\right)^2 \\[6pt]
    &= (110)(440) - (220)^2 \\[4pt]
    &= 48\,400 - 48\,400 \\[4pt]
    &= \boxed{0}
\end{align}

\noindent\textbf{Verificación directa con $\lambda = 2$:}

Dado que $x_{3i}=2x_{2i}$, las sumas satisfacen:
\[
  \sum x_{3i}^2 = \sum(2x_{2i})^2 = 4\sum x_{2i}^2 = 4\times110 = 440 \checkmark
\]
\[
  \sum x_{2i}x_{3i} = \sum x_{2i}(2x_{2i}) = 2\sum x_{2i}^2 = 2\times110 = 220 \checkmark
\]

Entonces:
\[
  D = \lambda^2\!\left(\sum x_{2i}^2\right)^2 - \lambda^2\!\left(\sum x_{2i}^2\right)^2
    = \lambda^2\!\left(\sum x_{2i}^2\right)^2\!(1-1) = 0 \checkmark
\]

\subsection{Coeficiente de Correlación \texorpdfstring{$r_{23}$}{r23}}

\begin{equation}
  r_{23} = \frac{\displaystyle\sum x_{2i}x_{3i}}{\displaystyle\sqrt{\sum x_{2i}^2\cdot\sum x_{3i}^2}}
  = \frac{220}{\sqrt{110\times440}}
  = \frac{220}{\sqrt{48\,400}}
  = \frac{220}{220}
  = 1
\end{equation}

El coeficiente de correlación es exactamente 1, confirmando la
\textbf{dependencia lineal perfecta positiva} entre $X_2$ y $X_3$.

\subsection{Aplicación de la Regla de Cramer: La Forma \texorpdfstring{$0/0$}{0/0}}

Intentemos aplicar la Regla de Cramer para obtener $\hat{\beta}_2$:

\begin{equation}
  \hat{\beta}_2
  = \frac{\left(\sxdy\right)\!\left(\sxtres\right)
          - \left(\sxty\right)\!\left(\sxdxt\right)}{D}
  = \frac{(220)(440) - (440)(220)}{0}
  = \frac{96\,800 - 96\,800}{0}
  = \frac{0}{0}
\end{equation}

Análogamente para $\hat{\beta}_3$:

\begin{equation}
  \hat{\beta}_3
  = \frac{\left(\sxty\right)\!\left(\sxdos\right)
          - \left(\sxdy\right)\!\left(\sxdxt\right)}{D}
  = \frac{(440)(110) - (220)(220)}{0}
  = \frac{48\,400 - 48\,400}{0}
  = \frac{0}{0}
\end{equation}

\begin{alerta}
La expresión $\dfrac{0}{0}$ es una \textbf{indeterminación matemática}: no posee un
valor definido.  No existe ningún método aritmético que permita resolver esta
operación.  El estimador MCO es, por tanto, \textbf{algebraicamente imposible de obtener}
con estos datos.
\end{alerta}

\subsection{Infinitas Soluciones: La Familia de Estimadores Equivalentes}

Para ilustrar que existen infinitas soluciones, observemos que cualquier par
$(\hat{\beta}_2,\hat{\beta}_3)$ que satisfaga la primera ecuación normal también
satisface la segunda (por ser la segunda múltiplo de la primera).

La Ecuación~A del sistema \eqref{eq:sistema2} es:
\[
  110\,\hat{\beta}_2 + 220\,\hat{\beta}_3 = 220
  \;\Longrightarrow\;
  \hat{\beta}_2 + 2\hat{\beta}_3 = 2
\]

Esta es una sola ecuación con dos incógnitas; su solución general es:

\begin{equation}
  \hat{\beta}_2 = 2 - 2\alpha, \qquad \hat{\beta}_3 = \alpha, \qquad \alpha\in\mathbb{R}
\end{equation}

Ejemplos particulares:

\vspace{0.4em}
\begin{center}
\begin{tabular}{ccc}
  \toprule
  \rowcolor{uptcBlue!15}
  $\alpha$ & $\hat{\beta}_2 = 2-2\alpha$ & $\hat{\beta}_3 = \alpha$ \\
  \midrule
  $0$   & $2$ & $0$ \\
  $1$   & $0$ & $1$ \\
  $1/2$ & $1$ & $1/2$ \\
  $-1$  & $4$ & $-1$ \\
  $5$   & $-8$ & $5$ \\
  \bottomrule
\end{tabular}
\end{center}

\vspace{0.4em}
\begin{corolario}
Todos los pares $(\hat{\beta}_2, \hat{\beta}_3) = (2-2\alpha,\,\alpha)$ con
$\alpha\in\mathbb{R}$ producen \textbf{exactamente el mismo valor ajustado}:
\[
  \hat{y}_i = (2-2\alpha)x_{2i} + \alpha x_{3i}
            = (2-2\alpha)x_{2i} + \alpha(2x_{2i})
            = 2x_{2i}
\]
La función de regresión estimada es única ($\hat{y}_i=2x_{2i}$), pero los
\textbf{coeficientes individuales son completamente indeterminados}: no hay base
estadística para elegir un par $({\hat{\beta}_2},{\hat{\beta}_3})$ sobre otro.
\end{corolario}

%=============================================================================
\section{Consecuencias Estadísticas Formales}
%=============================================================================

\subsection{Varianzas de los Estimadores y el Caso Límite}

Bajo el modelo clásico, la varianza del estimador $\hat{\beta}_2$ en el modelo
bivariado en desviaciones es:

\begin{equation}
  \Var(\hat{\beta}_2)
  = \frac{\sigma^2\displaystyle\sum x_{3i}^2}{D}
  = \frac{\sigma^2\displaystyle\sum x_{3i}^2}
         {\displaystyle\sum x_{2i}^2\cdot\sum x_{3i}^2 - \left(\sum x_{2i}x_{3i}\right)^2}
\end{equation}

Expresando el denominador en función del coeficiente de correlación
$r_{23}$ (usando la identidad
$D = \sum x_{2i}^2\cdot\sum x_{3i}^2\cdot(1-r_{23}^2)$):

\begin{equation}
  \Var(\hat{\beta}_2) = \frac{\sigma^2}{\displaystyle\sum x_{2i}^2\,(1 - r_{23}^2)}
\end{equation}

Cuando $r_{23}\to1$:
\[
  1 - r_{23}^2 \;\to\; 0
  \;\Longrightarrow\;
  \Var(\hat{\beta}_2) \;=\; \frac{\sigma^2}{\displaystyle\sum x_{2i}^2\cdot 0}
  \;\longrightarrow\; +\infty
\]

Para $r_{23}=1$ exactamente (nuestro caso):

\begin{equation}
  \Var(\hat{\beta}_2) = \frac{\sigma^2}{110\times(1-1)} = \frac{\sigma^2}{0} \to +\infty
\end{equation}

\begin{alerta}
\textbf{Con los datos del Problema~10.1:} $r_{23}=1$, $D=0$, luego
$\Var(\hat{\beta}_2)\to+\infty$ y $\Var(\hat{\beta}_3)\to+\infty$.
Los errores estándar son infinitos, los estadísticos $t$ colapsan a $0/\infty$,
y las pruebas de hipótesis carecen de todo sentido estadístico.
\end{alerta}

\subsection{Análisis Espectral de la Matriz \texorpdfstring{$\bm{M}$}{M}}

Los valores propios (eigenvalores) de $\bm{M}=\bm{X}'\bm{X}$ determinan su
invertibilidad.  Para nuestra matriz:
\[
  \bm{M}=\begin{pmatrix}110&220\\220&440\end{pmatrix}
\]

La ecuación característica $\det(\bm{M}-\mu\bm{I})=0$:
\begin{align}
  (110-\mu)(440-\mu) - 220^2 &= 0 \\
  \mu^2 - 550\mu + (48\,400 - 48\,400) &= 0 \\
  \mu^2 - 550\mu &= 0 \\
  \mu(\mu-550) &= 0
\end{align}

Los eigenvalores son $\mu_1 = 0$ y $\mu_2 = 550$.

\begin{resultado}[title=Interpretación espectral]
La presencia del eigenvalor $\mu_1=0$ confirma que $\bm{M}$ es
\textbf{semidefinida positiva y singular}: su determinante es
$\det(\bm{M})=\mu_1\cdot\mu_2=0\times550=0$, y su rango es
$\rg(\bm{M})=1 < 2 = k-1$.
El \textbf{número de condición} $\kappa = \mu_{\max}/\mu_{\min} = 550/0 = +\infty$
es la señal numérica definitiva de la singularidad.
\end{resultado}

\subsection{Resumen de las Tres Consecuencias Formales}

\begin{enumerate}[label=\textbf{\arabic*.}, leftmargin=*]

  \item \textbf{Indeterminación de los estimadores.}
    La operación $(\bm{X}'\bm{X})^{-1}$ no está definida.
    La Regla de Cramer conduce a la forma indeterminada $0/0$.
    Existen infinitas soluciones al sistema normal, todas produciendo el
    mismo $\hat{\bm{Y}}$ pero coeficientes arbitrariamente distintos.

  \item \textbf{Varianzas infinitas.}
    $\Var(\hat{\beta}_j) \to +\infty$ para $j=2,3$ cuando $r_{23}=1$.
    Los errores estándar no están definidos y los intervalos de confianza
    son $(-\infty,+\infty)$: ningún dato puede descartar ningún valor para
    $\beta_j$.

  \item \textbf{Fallo de identificación.}
    El modelo \emph{no está identificado}: la distribución conjunta de los
    datos es consistente con infinitos vectores $\bm{\beta}$,
    lo que hace imposible determinar cuánto de la variación de $Y$ se
    atribuye a $X_2$ y cuánto a $X_3$ de forma separada.

\end{enumerate}

%=============================================================================
\section{Generalización al Modelo de \texorpdfstring{$k$}{k} Variables}
%=============================================================================

\subsection{El Argumento General}

El resultado demostrado con $k=3$ variables se generaliza de forma directa a
cualquier modelo con $k$ variables explicativas.

\begin{prop}[Caso General]
  En el modelo $\bm{Y}=\bm{X\beta}+\bm{u}$ con $k$ columnas en $\bm{X}$,
  si existe $\bm{c}\neq\bm{0}$ tal que
  $X_k = \lambda_1 X_1 + \lambda_2 X_2 + \cdots + \lambda_{k-1} X_{k-1}$
  (al menos uno de los $\lambda_j$ no nulo), entonces:
  \begin{enumerate}[label=(\alph*)]
    \item Las $k$ columnas de $\bm{X}$ son linealmente dependientes.
    \item $\rg(\bm{X})<k$ y $\rg(\bm{X}'\bm{X})<k$.
    \item $\det(\bm{X}'\bm{X})=0$ y $(\bm{X}'\bm{X})^{-1}$ no existe.
    \item El sistema de $k$ ecuaciones normales tiene \emph{infinitas soluciones};
      ninguna puede elegirse como el estimador MCO.
    \item $\Var(\hat{\beta}_j)\to+\infty$ para todo $j$ afectado por la
      dependencia.
  \end{enumerate}
\end{prop}

\begin{dem}[Esquema general]
Las partes (a)--(d) se demuestran exactamente como en la Sección~4.  Para (e),
basta observar que la fórmula general de la varianza de $\hat{\beta}_j$
involucra el $(j,j)$-ésimo elemento diagonal de
$\sigma^2(\bm{X}'\bm{X})^{-1}$.  Cuando $\det(\bm{X}'\bm{X})=0$, los
elementos de la \emph{adjunta} divididos por el determinante divergen a $\pm\infty$.
\hfill$\square$
\end{dem}

\subsection{Las \texorpdfstring{$k$}{k} Ecuaciones Normales en Notación General}

El apéndice C de Gujarati \& Porter presenta el sistema de ecuaciones normales en
desviaciones para $k-1$ regresores como:

\begin{equation}
  \begin{pmatrix}
    \sum x_{2}^2 & \sum x_2x_3 & \cdots & \sum x_2x_k \\
    \sum x_2x_3 & \sum x_3^2  & \cdots & \sum x_3x_k \\
    \vdots       & \vdots       & \ddots & \vdots       \\
    \sum x_2x_k & \sum x_3x_k & \cdots & \sum x_k^2
  \end{pmatrix}
  \begin{pmatrix}\hat{\beta}_2\\\hat{\beta}_3\\\vdots\\\hat{\beta}_k\end{pmatrix}
  =
  \begin{pmatrix}\sum x_2 y\\\sum x_3 y\\\vdots\\\sum x_k y\end{pmatrix}
\end{equation}

Si $X_k = \sum_{j=1}^{k-1}\lambda_j X_j$, la última fila de la matriz de coeficientes
es una combinación lineal de las demás filas, haciendo que el sistema sea
\textbf{compatible indeterminado} (infinitas soluciones) en lugar de
\textbf{compatible determinado} (solución única), que es el caso requerido para MCO.

%=============================================================================
\section{Conclusión}
%=============================================================================

\begin{resultado}[title=Conclusión Final]
El conjunto de datos del Problema~10.1 satisface la relación de dependencia lineal
perfecta:
\[
  X_{3i} \;=\; 2X_{2i} - 1 \;\equiv\; -1\cdot X_{1i} + 2\cdot X_{2i}
\]
Como consecuencia directa:

\begin{enumerate}[leftmargin=*, label=\textbullet]
  \item El determinante de la matriz de momentos es $D=0$ (verificado
    numéricamente: $48\,400-48\,400=0$).
  \item El coeficiente de correlación entre los regresores es $r_{23}=1$.
  \item Los eigenvalores de $\bm{X}'\bm{X}$ son $\{0,\,550\}$; la presencia de
    $\mu=0$ confirma la singularidad.
  \item Los estimadores MCO resultan en formas indeterminadas $0/0$:
    \[
      \hat{\beta}_2 = \frac{0}{0}, \qquad \hat{\beta}_3 = \frac{0}{0}
    \]
  \item Las varianzas de los estimadores son $+\infty$.
  \item Existen \textbf{infinitas} combinaciones $(\hat{\beta}_2,\hat{\beta}_3)$
    que ajustan igualmente bien los datos; el modelo \textbf{no está identificado}.
\end{enumerate}

Queda así demostrado, tanto de forma \textbf{general} (por álgebra matricial y
la desigualdad de Cauchy-Schwarz) como de forma \textbf{particular} (cálculo
numérico explícito con los 11 datos), que la multicolinealidad perfecta hace
\textbf{algebraica y estadísticamente imposible} la estimación de los $k$
coeficientes de regresión mediante MCO.
\end{resultado}

\end{document}

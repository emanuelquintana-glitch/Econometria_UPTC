# Teoría de la Multicolinealidad: Análisis Matricial y Diagnóstico Completo

**Fecha:** Viernes, 13 de febrero de 2026  
**Entorno:** Gentoo Linux / Neovim  
**Materia:** Econometría Avanzada - UPTC  
**Autor:** Estudiante de Econometría

---

## Tabla de Contenidos

1. [Fundamentos Teóricos](#1-fundamentos-teóricos)
2. [Notación Matricial del Modelo](#2-notación-matricial-del-modelo)
3. [Matriz de Varianza-Covarianza](#3-matriz-de-varianza-covarianza)
4. [Diagnósticos de Multicolinealidad](#4-diagnósticos-de-multicolinealidad)
5. [Regresiones Auxiliares](#5-regresiones-auxiliares)
6. [Remedios y Soluciones](#6-remedios-y-soluciones)
7. [Aplicaciones Prácticas](#7-aplicaciones-prácticas)
8. [Referencias y Lecturas](#8-referencias-y-lecturas)

---

## 1. Fundamentos Teóricos

### 1.1 Definición Conceptual

La **multicolinealidad** ocurre cuando las variables explicativas en un modelo de regresión múltiple están altamente correlacionadas entre sí, lo que dificulta aislar el efecto individual de cada variable sobre la dependiente.

**Definición formal:**
> En el modelo lineal $Y = X\beta + U$, existe multicolinealidad cuando las columnas de la matriz $X$ son linealmente dependientes o casi dependientes.

### 1.2 Clasificación de la Multicolinealidad

#### A. Multicolinealidad Perfecta

**Definición matemática:**
$$\exists \lambda_1, \lambda_2, \ldots, \lambda_k \in \mathbb{R} \text{ (no todos cero) tal que } \sum_{j=1}^{k} \lambda_j X_j = 0$$

**Consecuencias:**

- El determinante $|X'X| = 0$
- La matriz $(X'X)$ es **singular** (no inversible)
- Los estimadores MCO **no existen** o son **indeterminados**
- Los errores estándar tienden a $\infty$

**Ejemplo típico: Trampa de la Variable Dummy**

```
Si incluimos:
- D₁ = 1 si es hombre, 0 si no
- D₂ = 1 si es mujer, 0 si no
- β₀ (intercepto)

Entonces: D₁ + D₂ = 1 (columna del intercepto)
→ Colinealidad perfecta
```

#### B. Multicolinealidad Aproximada o Imperfecta

**Definición:**
$$|X'X| \approx 0 \quad \text{(cercano a cero, pero no exactamente cero)}$$

**Características:**

- Los estimadores son **calculables** pero **imprecisos**
- Varianzas muy grandes
- Alta sensibilidad a cambios en los datos
- Problema de **grado**, no de existencia

### 1.3 Causas Comunes

| **Causa** | **Explicación** | **Ejemplo** |
|-----------|----------------|-------------|
| **Series de tiempo** | Las variables tienden a moverse juntas temporalmente | PIB, Consumo, Inversión crecen simultáneamente |
| **Especificación del modelo** | Inclusión de variables que miden fenómenos similares | Ingreso y Riqueza; Educación y Experiencia |
| **Micronumerosidad** | Tamaño de muestra pequeño relativo a parámetros | n=30, k=25 |
| **Métodos de recolección** | Muestreo en rango limitado | Solo observar familias de ingresos altos |
| **Términos polinomiales** | $X, X^2, X^3$ altamente correlacionados | Especialmente si $X$ tiene rango pequeño |

---

## 2. Notación Matricial del Modelo

### 2.1 Especificación General

Para un modelo de regresión lineal múltiple con $k$ variables explicativas:

$$Y = X\beta + U$$

**Donde:**

- $Y$: Vector $(n \times 1)$ de observaciones de la variable dependiente
  $$Y = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}$$

- $X$: Matriz de diseño $(n \times k)$
  $$X = \begin{bmatrix}
  1 & X_{12} & X_{13} & \cdots & X_{1k} \\
  1 & X_{22} & X_{23} & \cdots & X_{2k} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & X_{n2} & X_{n3} & \cdots & X_{nk}
  \end{bmatrix}$$

- $\beta$: Vector $(k \times 1)$ de parámetros poblacionales
  $$\beta = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}$$

- $U$: Vector $(n \times 1)$ de perturbaciones estocásticas
  $$U = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}$$

### 2.2 La Matriz de Información $(X'X)$

**Para un modelo con 3 variables ($k=3$):**

$$(X'X) = \begin{bmatrix}
n & \sum_{i=1}^{n} X_{i2} & \sum_{i=1}^{n} X_{i3} \\
\sum_{i=1}^{n} X_{i2} & \sum_{i=1}^{n} X_{i2}^2 & \sum_{i=1}^{n} X_{i2}X_{i3} \\
\sum_{i=1}^{n} X_{i3} & \sum_{i=1}^{n} X_{i2}X_{i3} & \sum_{i=1}^{n} X_{i3}^2
\end{bmatrix}$$

**Propiedades críticas:**

1. **Matriz simétrica:** $(X'X)' = X'X$
2. **Definida positiva** (bajo ausencia de multicolinealidad perfecta)
3. **Dimensión:** $(k \times k)$

### 2.3 El Estimador MCO

$$\hat{\beta} = (X'X)^{-1}X'Y$$

**Condición de existencia:**
$$(X'X)^{-1} \text{ existe } \iff |X'X| \neq 0 \iff \text{rango}(X) = k$$

**Efecto de la multicolinealidad:**

```
Si |X'X| ≈ 0:
→ Los elementos de (X'X)⁻¹ son muy grandes
→ β̂ es muy volátil
→ Pequeños cambios en Y provocan grandes cambios en β̂
```

### 2.4 Valores Predichos y Residuales

**Valores predichos:**
$$\hat{Y} = X\hat{\beta} = X(X'X)^{-1}X'Y = PY$$

Donde $P = X(X'X)^{-1}X'$ es la **matriz de proyección** o **matriz sombrero** (*hat matrix*).

**Residuales:**
$$\hat{U} = Y - \hat{Y} = Y - PY = (I - P)Y = MY$$

Donde $M = I - P$ es la **matriz aniquiladora** (*annihilator matrix*).

---

## 3. Matriz de Varianza-Covarianza

### 3.1 Definición General

La matriz de varianza-covarianza de los estimadores MCO se define como:

$$\text{Var-Cov}(\hat{\beta}) = \sigma^2(X'X)^{-1}$$

Donde $\sigma^2 = E(u_i^2)$ es la varianza del término de error.

### 3.2 Expansión para el Modelo Trivariado

Para el modelo $Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + u$:

$$\sigma^2(X'X)^{-1} = \begin{bmatrix}
\text{Var}(\hat{\beta}_1) & \text{Cov}(\hat{\beta}_1, \hat{\beta}_2) & \text{Cov}(\hat{\beta}_1, \hat{\beta}_3) \\
\text{Cov}(\hat{\beta}_2, \hat{\beta}_1) & \text{Var}(\hat{\beta}_2) & \text{Cov}(\hat{\beta}_2, \hat{\beta}_3) \\
\text{Cov}(\hat{\beta}_3, \hat{\beta}_1) & \text{Cov}(\hat{\beta}_3, \hat{\beta}_2) & \text{Var}(\hat{\beta}_3)
\end{bmatrix}$$

**Propiedades:**
- Matriz **simétrica**: $\text{Cov}(\hat{\beta}_i, \hat{\beta}_j) = \text{Cov}(\hat{\beta}_j, \hat{\beta}_i)$
- Elementos de la diagonal: **varianzas**
- Elementos fuera de la diagonal: **covarianzas**

### 3.3 El Fenómeno de "Explosión de Varianza"

**Fórmula clave para la varianza de un coeficiente individual:**

$$\text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2} \cdot \frac{1}{1 - R_j^2}$$

**Descomposición:**

$$\text{Var}(\hat{\beta}_j) = \underbrace{\frac{\sigma^2}{\text{STC}_j}}_{\text{Componente base}} \times \underbrace{\frac{1}{1 - R_j^2}}_{\text{FIV}_j}$$

Donde:
- $\text{STC}_j = \sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2$ = Suma Total de Cuadrados de $X_j$
- $R_j^2$ = Coeficiente de determinación de la **regresión auxiliar** de $X_j$ sobre todas las demás $X$

**Análisis del Factor de Inflación:**

| $R_j^2$ | $\text{FIV}_j$ | Interpretación |
|---------|----------------|----------------|
| 0.00 | 1.00 | Sin correlación (ideal) |
| 0.50 | 2.00 | Duplica la varianza |
| 0.80 | 5.00 | Quintuplica la varianza |
| 0.90 | 10.00 | **Umbral crítico** |
| 0.95 | 20.00 | Problema severo |
| 0.99 | 100.00 | **Problema catastrófico** |
| 1.00 | $\infty$ | Colinealidad perfecta |

**Consecuencia directa:**

Si $R_j^2 \to 1$:
$$\text{Var}(\hat{\beta}_j) \to \infty$$

Esto invalida las pruebas de hipótesis $t$, ya que:

$$t = \frac{\hat{\beta}_j}{\text{se}(\hat{\beta}_j)} = \frac{\hat{\beta}_j}{\sqrt{\text{Var}(\hat{\beta}_j)}} \to 0$$

### 3.4 Estimación Práctica

En la práctica, $\sigma^2$ es desconocido y se estima mediante:

$$\hat{\sigma}^2 = s^2 = \frac{\sum_{i=1}^{n} \hat{u}_i^2}{n - k} = \frac{\text{SRC}}{n - k}$$

Por lo tanto:

$$\widehat{\text{Var-Cov}}(\hat{\beta}) = s^2(X'X)^{-1}$$

**Los errores estándar son:**

$$\text{se}(\hat{\beta}_j) = \sqrt{s^2 \cdot [(X'X)^{-1}]_{jj}}$$

---

## 4. Diagnósticos de Multicolinealidad

### 4.1 Síntomas Observables

#### A. La Paradoja Clásica: $R^2$ Alto con Coeficientes No Significativos

**Configuración típica:**

```
Modelo: Y = β₁ + β₂X₂ + β₃X₃ + β₄X₄ + u

Resultados:
- R² = 0.95 (muy alto)
- Prueba F: Significativa al 1% → El modelo funciona globalmente
- Prueba t para β₂: No significativa (p > 0.10)
- Prueba t para β₃: No significativa (p > 0.10)
- Prueba t para β₄: No significativa (p > 0.10)

→ DIAGNÓSTICO: Multicolinealidad severa
```

**Explicación matemática:**

El estadístico $F$ prueba:
$$H_0: \beta_2 = \beta_3 = \beta_4 = 0$$

$$F = \frac{R^2/(k-1)}{(1-R^2)/(n-k)}$$

Este puede ser grande incluso cuando los estadísticos $t$ individuales son pequeños debido a las **altas covarianzas** entre los estimadores.

#### B. Sensibilidad Extrema a Cambios en los Datos

**Ejemplo ilustrativo:**

**Dataset Original (n=50):**
$$\hat{Y} = 10.5 + 2.3X_2 - 1.8X_3$$
$$(t)$$ $$(5.2)$$ $$(1.1)$$ $$(-0.9)$$

**Dataset con 1 observación adicional (n=51):**
$$\hat{Y} = 8.2 - 0.5X_2 + 3.1X_3$$
$$(t)$$ $$(4.1)$$ $$(-0.3)$$ $$(1.4)$$

**¡Los signos cambiaron completamente!** Esto es inadmisible.

#### C. Signos Económicamente Contradictorios

**Ejemplo: Función de Demanda**

Teoría económica predice: $\frac{\partial Q}{\partial P} < 0$ (ley de la demanda)

```
Modelo estimado: Q̂ = 100 + 5P - 3I
                       (↑)
                    INCORRECTO

Donde:
- Q = Cantidad demandada
- P = Precio
- I = Ingreso

→ El coeficiente de P es POSITIVO (viola la teoría)
→ Posible causa: P e I altamente correlacionados
```

### 4.2 Factor de Inflación de Varianza (FIV/VIF)

#### Definición y Cálculo

**Fórmula:**
$$\text{FIV}_j = \frac{1}{1 - R_j^2}$$

Donde $R_j^2$ se obtiene de la **regresión auxiliar:**

$$X_j = \delta_0 + \delta_1 X_1 + \delta_2 X_2 + \cdots + \delta_{j-1} X_{j-1} + \delta_{j+1} X_{j+1} + \cdots + \delta_k X_k + v_j$$

**Procedimiento paso a paso:**

1. Para cada variable $X_j$ ($j = 2, 3, \ldots, k$):
   - Regresar $X_j$ sobre todas las demás $X_i$ ($i \neq j$)
   - Obtener $R_j^2$ de esta regresión

2. Calcular:
   $$\text{FIV}_j = \frac{1}{1 - R_j^2}$$

3. Interpretar según criterios

#### Criterios de Decisión

| **Indicador** | **Valor** | **Diagnóstico** | **Acción** |
|---------------|-----------|-----------------|------------|
| $\text{FIV}_j$ | $< 5$ | Colinealidad leve | Ninguna acción necesaria |
| $\text{FIV}_j$ | $5 - 10$ | Colinealidad moderada | Monitorear con cuidado |
| $\text{FIV}_j$ | $> 10$ | **Colinealidad grave** | **Acción correctiva urgente** |
| $\text{FIV}_j$ | $> 100$ | Colinealidad catastrófica | Rediseñar modelo completamente |

**Nota:** Algunos autores usan el umbral de 5, otros 10. En contextos de series de tiempo, umbrales más altos (20-30) son tolerables.

#### Ejemplo Numérico

```
Supongamos un modelo con 4 variables:
Y = β₁ + β₂X₂ + β₃X₃ + β₄X₄ + u

Paso 1: Regresiones auxiliares
- X₂ sobre X₃, X₄ → R₂² = 0.92
- X₃ sobre X₂, X₄ → R₃² = 0.45
- X₄ sobre X₂, X₃ → R₄² = 0.88

Paso 2: Calcular FIV
- FIV₂ = 1/(1-0.92) = 12.50 → GRAVE
- FIV₃ = 1/(1-0.45) = 1.82  → Aceptable
- FIV₄ = 1/(1-0.88) = 8.33  → Moderado-Alto

Paso 3: Diagnóstico
→ X₂ presenta multicolinealidad SEVERA
→ X₄ presenta multicolinealidad MODERADA
→ X₃ es relativamente independiente
```

### 4.3 Factor de Tolerancia (TOL)

**Definición:**
$$\text{TOL}_j = \frac{1}{\text{FIV}_j} = 1 - R_j^2$$

**Interpretación:**

| $\text{TOL}_j$ | Interpretación |
|----------------|----------------|
| $\to 1$ | Independencia total (ortogonalidad) |
| $> 0.20$ | Aceptable |
| $0.10 - 0.20$ | Zona de precaución |
| $< 0.10$ | **Problema serio** |
| $\to 0$ | Colinealidad perfecta |

**Relación con FIV:**

$$\begin{aligned}
\text{TOL} = 0.10 &\iff \text{FIV} = 10 \\
\text{TOL} = 0.05 &\iff \text{FIV} = 20 \\
\text{TOL} = 0.01 &\iff \text{FIV} = 100
\end{aligned}$$

### 4.4 Matriz de Correlación de Variables Explicativas ($R_{XX}$)

#### Estructura

Para $k$ variables explicativas (excluyendo el intercepto):

$$R_{XX} = \begin{bmatrix}
1 & r_{23} & r_{24} & \cdots & r_{2k} \\
r_{32} & 1 & r_{34} & \cdots & r_{3k} \\
r_{42} & r_{43} & 1 & \cdots & r_{4k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{k2} & r_{k3} & r_{k4} & \cdots & 1
\end{bmatrix}$$

Donde el coeficiente de correlación simple es:

$$r_{ij} = \frac{\sum_{t=1}^{n} (X_{ti} - \bar{X}_i)(X_{tj} - \bar{X}_j)}{\sqrt{\sum_{t=1}^{n} (X_{ti} - \bar{X}_i)^2} \sqrt{\sum_{t=1}^{n} (X_{tj} - \bar{X}_j)^2}}$$

#### Diagnóstico basado en el Determinante

**Interpretación de $|R_{XX}|$:**

| $|R_{XX}|$ | Diagnóstico |
|-----------|-------------|
| $= 1$ | Ortogonalidad perfecta (correlaciones todas cero) |
| $> 0.5$ | Colinealidad leve |
| $0.1 - 0.5$ | Colinealidad moderada |
| $< 0.1$ | Colinealidad severa |
| $\approx 0$ | Colinealidad muy severa |
| $= 0$ | Colinealidad perfecta |

#### Regla Práctica: Correlaciones Simples

**Umbral tradicional:**
$$|r_{ij}| > 0.80 \implies \text{Bandera roja}$$

**⚠️ ADVERTENCIA CRÍTICA:**

Esta regla es **necesaria pero NO suficiente**. Puede existir multicolinealidad severa incluso con todas las correlaciones simples bajas:

```
Ejemplo:
r₂₃ = 0.3, r₂₄ = 0.4, r₃₄ = 0.35

Pero si: X₂ = 2X₃ + 3X₄ + ε (con ε pequeño)
→ Multicolinealidad múltiple NO detectada por correlaciones simples
```

### 4.5 Test de Farrar-Glauber

Es un procedimiento de **tres etapas** para detectar, localizar y cuantificar multicolinealidad.

#### Etapa 1: Test Chi-Cuadrado (Detección Global)

**Estadístico de Bartlett:**

$$\chi^2_{\text{calc}} = -\left[ n - 1 - \frac{1}{6}(2k + 5) \right] \ln |R_{XX}|$$

**Hipótesis:**
- $H_0$: Las variables explicativas son ortogonales (no hay multicolinealidad)
- $H_1$: Existe multicolinealidad

**Distribución bajo $H_0$:**
$$\chi^2_{\text{calc}} \sim \chi^2_{\left[\frac{k(k-1)}{2}\right]}$$

Donde los grados de libertad son:
$$gl = \frac{1}{2}k(k-1)$$

**Regla de decisión:**
$$\text{Rechazar } H_0 \text{ si } \chi^2_{\text{calc}} > \chi^2_{\alpha, gl}$$

#### Ejemplo Numérico Completo

**Datos:**
- $n = 50$ observaciones
- $k = 3$ variables explicativas (excluyendo intercepto)
- $|R_{XX}| = 0.10$

**Paso 1: Calcular el factor de corrección**

$$\begin{aligned}
\text{Factor} &= n - 1 - \frac{1}{6}(2k + 5) \\
&= 50 - 1 - \frac{1}{6}(2 \times 3 + 5) \\
&= 49 - \frac{11}{6} \\
&= 49 - 1.833 \\
&= 47.167
\end{aligned}$$

**Paso 2: Calcular el estadístico**

$$\begin{aligned}
\chi^2_{\text{calc}} &= -47.167 \times \ln(0.10) \\
&= -47.167 \times (-2.303) \\
&= 108.65
\end{aligned}$$

**Paso 3: Determinar grados de libertad**

$$gl = \frac{k(k-1)}{2} = \frac{3(3-1)}{2} = 3$$

**Paso 4: Valor crítico**

Para $\alpha = 0.05$ y $gl = 3$:
$$\chi^2_{0.05, 3} = 7.815$$

**Paso 5: Decisión**

$$\chi^2_{\text{calc}} = 108.65 > 7.815 = \chi^2_{\text{crítico}}$$

**Conclusión:** Se rechaza $H_0$ → **Existe multicolinealidad significativa**

#### Etapa 2: Test F (Localización de Variables Problemáticas)

Para cada variable $X_j$, ejecutar la regresión auxiliar:

$$X_j = \delta_0 + \sum_{i \neq j} \delta_i X_i + v_j$$

**Estadístico F:**
$$F_j = \frac{R_j^2 / (k-2)}{(1 - R_j^2) / (n - k + 1)}$$

**Distribución bajo $H_0$ (variable no es colineal):**
$$F_j \sim F_{(k-2, n-k+1)}$$

**Interpretación:**
- Si $F_j$ es **significativo** → $X_j$ es altamente colineal con otras
- Si $F_j$ es **no significativo** → $X_j$ no presenta colinealidad problemática

#### Etapa 3: Test t (Identificación de Pares Colineales)

**Correlación parcial entre $X_i$ y $X_j$ controlando por las demás:**

$$r_{ij \cdot \text{resto}} = \frac{-c_{ij}}{\sqrt{c_{ii} c_{jj}}}$$

Donde $c_{ij}$ son los elementos de la matriz $(R_{XX})^{-1}$.

**Estadístico t:**
$$t_{ij} = r_{ij \cdot \text{resto}} \sqrt{\frac{n - k}{1 - r_{ij \cdot \text{resto}}^2}}$$

**Distribución:** $t_{ij} \sim t_{(n-k)}$

**Si $t_{ij}$ es significativo** → El par $(X_i, X_j)$ está altamente correlacionado

### 4.6 Análisis de Valores Propios (Eigenvalues)

#### Descomposición Espectral

La matriz $(X'X)$ (o su versión estandarizada) puede descomponerse como:

$$(X'X) = Q\Lambda Q'$$

Donde:
- $Q$: Matriz ortogonal $(k \times k)$ de vectores propios
- $\Lambda = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_k)$: Matriz diagonal de valores propios
- $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k \geq 0$

#### Número de Condición ($\kappa$)

$$\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}$$

**Criterios:**

| $\kappa$ | Diagnóstico |
|----------|-------------|
| $< 100$ | No hay problema |
| $100 - 1000$ | Colinealidad moderada a fuerte |
| $> 1000$ | Colinealidad severa |
| $> 10000$ | Colinealidad extrema |

#### Índice de Condición (CI)

$$\text{CI} = \sqrt{\kappa} = \sqrt{\frac{\lambda_{\max}}{\lambda_{\min}}}$$

**Criterios de Belsley, Kuh y Welsch:**

| $\text{CI}$ | Diagnóstico |
|-------------|-------------|
| $< 10$ | Sin multicolinealidad |
| $10 - 30$ | **Multicolinealidad moderada a fuerte** |
| $\geq 30$ | **Multicolinealidad severa** |

#### Descomposición de Varianza (Variance Decomposition Proportions)

Para cada valor propio $\lambda_l$, calculamos la proporción de la varianza del coeficiente $\beta_j$ asociada con ese valor propio:

$$\phi_{jl} = \frac{(q_{jl})^2 / \lambda_l}{\sum_{m=1}^{k} (q_{jm})^2 / \lambda_m}$$

Donde $q_{jl}$ es el elemento $(j,l)$ de la matriz $Q$.

**Regla de diagnóstico:**

Si dos o más coeficientes tienen proporciones altas ($> 0.5$) asociadas con un valor propio pequeño, esas variables están colineales.

**Ejemplo:**

```
Valor propio λ₃ = 0.05 (pequeño)

Descomposición de varianza asociada a λ₃:
- φ₂,₃ = 0.85 (variable X₂)
- φ₄,₃ = 0.92 (variable X₄)

→ DIAGNÓSTICO: X₂ y X₄ están altamente colineales
```

---

## 5. Regresiones Auxiliares

### 5.1 Procedimiento General

**Objetivo:** Identificar cuál(es) variable(s) es(son) la(s) responsable(s) de la multicolinealidad.

**Método:** Ejecutar $k-1$ regresiones donde cada $X_j$ ($j = 2, 3, \ldots, k$) actúa temporalmente como variable dependiente.

#### Para $X_2$:

$$X_2 = \gamma_1 + \gamma_3 X_3 + \gamma_4 X_4 + \cdots + \gamma_k X_k + v_2$$

- Obtener $R_2^2$
- Calcular $\text{FIV}_2 = \frac{1}{1 - R_2^2}$

#### Para $X_3$:

$$X_3 = \delta_1 + \delta_2 X_2 + \delta_4 X_4 + \cdots + \delta_k X_k + v_3$$

- Obtener $R_3^2$
- Calcular $\text{FIV}_3 = \frac{1}{1 - R_3^2}$

#### Y así sucesivamente para todas las variables

### 5.2 Matriz Resumen de Diagnóstico

| **Variable** | **$R_j^2$** | **FIV** | **TOL** | **Diagnóstico** | **Acción** |
|--------------|-------------|---------|---------|-----------------|------------|
| $X_2$ | 0.92 | 12.50 | 0.08 | **Altamente colineal** | Eliminar o transformar |
| $X_3$ | 0.45 | 1.82 | 0.55 | Aceptable | Mantener |
| $X_4$ | 0.88 | 8.33 | 0.12 | **Colineal** | Considerar eliminación |
| $X_5$ | 0.15 | 1.18 | 0.85 | Sin problema | Mantener |

### 5.3 Regla de Klein

**Enunciado:**

La multicolinealidad es problemática si:
$$R_j^2 > R^2$$

Donde:
- $R_j^2$: $R^2$ de la regresión auxiliar de $X_j$
- $R^2$: $R^2$ del modelo original completo

**Interpretación:**

Si la regresión auxiliar explica **más** variación de $X_j$ que el modelo original explica de $Y$, entonces $X_j$ es redundante.

**Ejemplo:**

```
Modelo original: Ŷ = β₁ + β₂X₂ + β₃X₃ + β₄X₄
R² = 0.75

Regresión auxiliar: X̂₂ = γ₁ + γ₃X₃ + γ₄X₄
R₂² = 0.88

Como R₂² (0.88) > R² (0.75):
→ X₂ es altamente colineal según el criterio de Klein
```

### 5.4 Prueba F para Regresión Auxiliar

**Hipótesis:**
- $H_0$: $X_j$ no está correlacionada con las demás (todos los $\gamma_i = 0$)
- $H_1$: $X_j$ está correlacionada con al menos una variable

**Estadístico:**
$$F = \frac{R_j^2 / (k-2)}{(1 - R_j^2) / (n - k + 1)}$$

**Decisión:**

Si $F > F_{\alpha, (k-2, n-k+1)}$ → Rechazar $H_0$ → $X_j$ es colineal

---

## 6. Remedios y Soluciones

### 6.1 Filosofía General

**Pregunta fundamental:** ¿Hacer algo o no hacer nada?

**La respuesta depende del objetivo:**

| **Objetivo** | **¿Importa la multicolinealidad?** | **Acción** |
|--------------|-----------------------------------|------------|
| **Predicción** | NO (si se mantiene la estructura) | No hacer nada |
| **Inferencia** | SÍ (coeficientes imprecisos) | Aplicar remedios |
| **Interpretación económica** | SÍ (signos incorrectos, magnitudes) | Aplicar remedios |

**Recordatorio crítico:**

> La multicolinealidad NO sesga los estimadores MCO. Son **insesgados** pero **ineficientes** (alta varianza).

### 6.2 Opción 1: No Hacer Nada

**Cuándo es apropiado:**

1. **El objetivo principal es predicción:**
   - Si $\hat{Y} = X\hat{\beta}$ predice bien
   - Y se espera que la estructura de correlación se mantenga en el futuro
   - Los coeficientes individuales son irrelevantes

2. **La colinealidad es inherente al fenómeno:**
   - Ejemplo: Altura y Peso están naturalmente correlacionados
   - No tiene sentido "resolver" algo que es parte de la realidad

3. **El $R^2$ es suficientemente alto para el propósito:**
   - Si solo se necesita medir bondad de ajuste global

**Riesgos:**
- Intervalos de confianza amplios
- Predicciones fuera de muestra pueden fallar si la estructura cambia

### 6.3 Opción 2: Información A Priori (Restricciones Paramétricas)

**Idea:** Usar conocimiento teórico o evidencia empírica previa para imponer restricciones en los parámetros.

#### Ejemplo 1: Función de Producción Cobb-Douglas con Rendimientos Constantes a Escala

**Modelo sin restricción:**
$$\ln Q = \beta_1 + \beta_2 \ln K + \beta_3 \ln L + u$$

**Problema:** $\ln K$ y $\ln L$ altamente correlacionados

**Solución:** Si sabemos por teoría que hay **rendimientos constantes a escala**:
$$\beta_2 + \beta_3 = 1$$

**Imposición:**
$$\beta_3 = 1 - \beta_2$$

**Sustituyendo:**
$$\begin{aligned}
\ln Q &= \beta_1 + \beta_2 \ln K + (1 - \beta_2) \ln L + u \\
\ln Q &= \beta_1 + \beta_2 \ln K + \ln L - \beta_2 \ln L + u \\
\ln Q - \ln L &= \beta_1 + \beta_2 (\ln K - \ln L) + u \\
\ln(Q/L) &= \beta_1 + \beta_2 \ln(K/L) + u
\end{aligned}$$

**Modelo final (una sola variable explicativa):**
$$\ln(\text{Productividad}) = \beta_1 + \beta_2 \ln(\text{Intensidad de capital}) + u$$

**Ventaja:** Multicolinealidad completamente eliminada

#### Ejemplo 2: Modelo de Inflación con Neutralidad del Dinero

**Modelo:**
$$\dot{P}_t = \alpha + \sum_{i=0}^{n} m_i \dot{M}_{t-i} + u_t$$

**Problema:** Los rezagos de $\dot{M}$ están correlacionados

**Restricción teórica:** Neutralidad del dinero a largo plazo
$$\sum_{i=0}^{n} m_i = 1$$

**Implementación:**
$$m_n = 1 - \sum_{i=0}^{n-1} m_i$$

Esto reduce el número de parámetros libres en 1.

### 6.4 Opción 3: Transformación de Variables

#### A. Primeras Diferencias

**Útil para:** Series de tiempo con tendencias comunes

**Modelo original en niveles:**
$$Y_t = \beta_1 + \beta_2 X_{2t} + \beta_3 X_{3t} + u_t$$

**Problema:** $X_{2t}$ y $X_{3t}$ crecen simultáneamente (tendencia común)

**Solución:** Tomar primeras diferencias

$$\Delta Y_t = Y_t - Y_{t-1}$$
$$\Delta X_{jt} = X_{jt} - X_{jt-1}$$

**Modelo en diferencias:**
$$\Delta Y_t = \beta_2 \Delta X_{2t} + \beta_3 \Delta X_{3t} + \Delta u_t$$

**Ventajas:**
- Elimina tendencias comunes (determinísticas o estocásticas)
- Reduce correlación entre regresores

**Desventajas:**
- Pérdida de información sobre niveles
- Pérdida de una observación ($t=1$)
- Puede introducir autocorrelación en $\Delta u_t$ si $u_t \sim \text{MA}(1)$

#### B. Razones o Cocientes

**Idea:** Convertir variables en ratios

**Ejemplo 1: Función de Producción**

En lugar de:
$$Q = \beta_1 + \beta_2 K + \beta_3 L + u$$

Dividir todo entre $L$:
$$\frac{Q}{L} = \beta_1 \frac{1}{L} + \beta_2 \frac{K}{L} + \beta_3 + u/L$$

**Ejemplo 2: Variables de Tamaño**

En lugar de:
- Ventas ($S$) e Inversión en Publicidad ($A$)

Usar:
$$\text{Eficiencia Publicitaria} = \frac{S}{A}$$

**Ventaja:** Reduce el efecto de escala

**Desventaja:** Puede inducir heterocedasticidad en el nuevo término de error

#### C. Desviaciones respecto a la Media (Centrar Variables)

**Para modelos con términos polinomiales:**

En lugar de:
$$Y = \beta_1 + \beta_2 X + \beta_3 X^2 + u$$

Definir:
$$x^* = X - \bar{X}$$

**Modelo centrado:**
$$Y = \beta_1^* + \beta_2^* x^* + \beta_3^* (x^*)^2 + u$$

**Ventaja:** Reduce (no elimina) correlación entre $x^*$ y $(x^*)^2$

### 6.5 Opción 4: Eliminación de Variables

**Criterio:** Identificar la variable con el **FIV más alto** y eliminarla

**Procedimiento iterativo:**

1. Calcular FIV para todas las variables
2. Identificar $\text{FIV}_{\max}$
3. Si $\text{FIV}_{\max} > 10$:
   - Eliminar esa variable
   - Re-estimar el modelo
   - Recalcular FIV
   - Repetir hasta que todos $\text{FIV}_j < 10$

**⚠️ ADVERTENCIA CRÍTICA:**

Esta estrategia conlleva el riesgo de **sesgo de especificación** (omisión de variable relevante).

**Consecuencia del sesgo de especificación:**

Si el modelo verdadero es:
$$Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + u$$

Pero estimamos (omitiendo $X_3$):
$$Y = \beta_1 + \beta_2 X_2 + v$$

Entonces:
$$E(\hat{\beta}_2) = \beta_2 + \beta_3 \cdot \frac{\text{Cov}(X_2, X_3)}{\text{Var}(X_2)}$$

**El estimador es SESGADO** (viola el teorema de Gauss-Markov).

**Regla de decisión:**

Solo eliminar si:
- La variable es **teóricamente redundante**
- Existe **información a priori** que justifique su exclusión
- El coste del sesgo es menor que el coste de la imprecisión

### 6.6 Opción 5: Incrementar el Tamaño de la Muestra

**Fundamento matemático:**

Recordemos:
$$\text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2} \cdot \text{FIV}_j$$

Para una muestra aleatoria donde $\text{Var}(X_j)$ es constante:
$$\sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2 \approx n \cdot \text{Var}(X_j)$$

Por lo tanto:
$$\text{Var}(\hat{\beta}_j) \approx \frac{\sigma^2}{n \cdot \text{Var}(X_j)} \cdot \text{FIV}_j$$

**Al aumentar $n$:**
$$\text{Var}(\hat{\beta}_j) \propto \frac{1}{n}$$

**Consecuencias:**
- Los errores estándar disminuyen: $\text{se}(\hat{\beta}_j) \propto \frac{1}{\sqrt{n}}$
- Los estadísticos $t$ aumentan: $t = \frac{\hat{\beta}_j}{\text{se}(\hat{\beta}_j)} \propto \sqrt{n}$
- Los intervalos de confianza se estrechan

**Limitaciones prácticas:**
- No siempre es posible obtener más datos
- Costo económico/temporal
- El FIV permanece constante (la estructura de correlación no cambia)

**Regla práctica:**

Para reducir el error estándar a la mitad:
$$n_{\text{nuevo}} = 4 \times n_{\text{original}}$$

### 6.7 Opción 6: Técnicas Avanzadas

#### A. Regresión Ridge

**Problema con MCO:**

Cuando $(X'X)$ está cerca de ser singular, $(X'X)^{-1}$ tiene elementos muy grandes.

**Solución Ridge:**

En lugar de minimizar:
$$\text{SRC} = (Y - X\beta)'(Y - X\beta)$$

Minimizar:
$$\text{SRC}_{\text{Ridge}} = (Y - X\beta)'(Y - X\beta) + \lambda \beta'\beta$$

Donde $\lambda > 0$ es el **parámetro de penalización** o **constante de Ridge**.

**Estimador Ridge:**
$$\hat{\beta}_{\text{Ridge}} = (X'X + \lambda I)^{-1}X'Y$$

**Propiedades:**

1. $(X'X + \lambda I)$ siempre es invertible (incluso si $(X'X)$ es singular)
2. $\hat{\beta}_{\text{Ridge}}$ es **sesgado** pero tiene **menor varianza** que MCO
3. Trade-off sesgo-varianza controlado por $\lambda$

**Selección de $\lambda$:**

- **Ridge Trace:** Graficar $\hat{\beta}_{\text{Ridge}}(\lambda)$ vs. $\lambda$
- **Validación cruzada:** Minimizar error de predicción
- **Criterio de información:** AIC, BIC

**Ventaja principal:**

Reduce simultáneamente la varianza de **todos** los coeficientes

**Desventaja:**

No produce coeficientes exactamente cero → No hace selección de variables

#### B. LASSO (Least Absolute Shrinkage and Selection Operator)

**Diferencia con Ridge:**

Penaliza la norma $L_1$ en lugar de $L_2$:

$$\text{SRC}_{\text{LASSO}} = (Y - X\beta)'(Y - X\beta) + \lambda \sum_{j=1}^{k} |\beta_j|$$

**Estimador LASSO:**

No tiene solución analítica cerrada. Se resuelve mediante algoritmos iterativos (ej. LARS, coordinate descent).

**Ventaja sobre Ridge:**

Puede forzar coeficientes exactamente a **cero** → Realiza **selección de variables**

#### C. Elastic Net

**Combinación de Ridge y LASSO:**

$$\text{SRC}_{\text{EN}} = (Y - X\beta)'(Y - X\beta) + \lambda_1 \sum_{j=1}^{k} |\beta_j| + \lambda_2 \beta'\beta$$

**Ventaja:**

Hereda las propiedades de selección de LASSO y la estabilidad de Ridge

#### D. Análisis de Componentes Principales (PCA)

**Idea:** Transformar las $k$ variables correlacionadas en $m$ componentes principales ortogonales ($m < k$)

**Procedimiento:**

1. Estandarizar las variables $X$
2. Calcular la matriz de correlación $R_{XX}$
3. Obtener valores propios $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_k$ y vectores propios correspondientes
4. Construir componentes principales:
   $$PC_j = \sum_{i=1}^{k} a_{ij} X_i$$

   Donde $a_{ij}$ son los elementos del vector propio correspondiente a $\lambda_j$

5. Regresar $Y$ sobre los primeros $m$ componentes (aquellos con $\lambda_j$ grande):
   $$Y = \gamma_1 PC_1 + \gamma_2 PC_2 + \cdots + \gamma_m PC_m + u$$

**Ventaja:**

Elimina **completamente** la multicolinealidad (componentes principales son ortogonales)

**Desventaja crítica:**

Pérdida de **interpretabilidad económica**:

$$\hat{\beta}_{\text{original}} \neq \text{función simple de } \hat{\gamma}$$

La transformación inversa es compleja y los coeficientes originales reconstruidos son difíciles de interpretar.

### 6.8 Cuadro Comparativo de Remedios

| **Remedio** | **Ventajas** | **Desventajas** | **Cuándo usar** |
|-------------|--------------|-----------------|-----------------|
| **No hacer nada** | Estimadores insesgados; Simple | Baja precisión | Objetivo: Predicción |
| **Información a priori** | Elimina completamente el problema; Teoría respetada | Requiere conocimiento teórico sólido | Disponible restricción teórica |
| **Primeras diferencias** | Elimina tendencias; Reduce correlación | Pérdida de info de niveles; Posible autocorrelación | Series de tiempo con tendencias |
| **Razones** | Controla efecto escala | Puede inducir heterocedasticidad | Variables de tamaño |
| **Eliminar variables** | Simple; Reduce dimensión | **Riesgo de sesgo** | Variable redundante teóricamente |
| **Más datos** | Reduce error estándar sin sesgo | Costoso/Imposible | Factible conseguir más observaciones |
| **Ridge** | Reduce varianza; Siempre invertible | Introduce sesgo; No selecciona variables | Predicción; Multicolinealidad severa |
| **LASSO** | Selección automática; Reduce varianza | Introduce sesgo; Inestable con grupos correlacionados | Selección de variables |
| **PCA** | Ortogonalidad perfecta | **Pérdida interpretabilidad** | Objetivo exclusivamente predictivo |

---

## 7. Aplicaciones Prácticas

### 7.1 Modelo de Inflación Monetarista

#### Especificación del Modelo

Según el enfoque monetarista, la inflación ($\dot{P}_t$) es función de la tasa de crecimiento de la oferta monetaria:

$$\dot{P}_t = \alpha + \sum_{i=0}^{n} m_i \dot{M}_{t-i} + u_t$$

**Variables:**
- $\dot{P}_t$: Tasa de cambio del nivel de precios (IPC o deflactor del PNB)
- $\dot{M}_{t-i}$: Tasa de crecimiento de M1 en el período $t-i$
- $m_i$: Multiplicadores (elasticidades precio-dinero)
- $n$: Número de rezagos (empíricamente 3-20 trimestres)

#### Problema de Multicolinealidad

**Fuente:**

Las tasas de crecimiento del dinero en períodos consecutivos están altamente correlacionadas:

$$\text{Corr}(\dot{M}_t, \dot{M}_{t-1}) \approx 0.85$$
$$\text{Corr}(\dot{M}_{t-1}, \dot{M}_{t-2}) \approx 0.82$$

**Consecuencia:**

Los FIV de los coeficientes $m_i$ son muy altos ($> 20$)

#### Solución: Restricciones Polinomiales (Almon Lag)

**Idea:** Imponer que los coeficientes $m_i$ sigan un polinomio de grado bajo:

$$m_i = \gamma_0 + \gamma_1 i + \gamma_2 i^2$$

**Para un rezago de orden 2:**

$$\begin{aligned}
m_0 &= \gamma_0 \\
m_1 &= \gamma_0 + \gamma_1 + \gamma_2 \\
m_2 &= \gamma_0 + 2\gamma_1 + 4\gamma_2 \\
&\vdots
\end{aligned}$$

**Sustituyendo en el modelo:**

$$\begin{aligned}
\dot{P}_t &= \alpha + \sum_{i=0}^{n} (\gamma_0 + \gamma_1 i + \gamma_2 i^2) \dot{M}_{t-i} + u_t \\
&= \alpha + \gamma_0 \sum_{i=0}^{n} \dot{M}_{t-i} + \gamma_1 \sum_{i=0}^{n} i \cdot \dot{M}_{t-i} + \gamma_2 \sum_{i=0}^{n} i^2 \cdot \dot{M}_{t-i} + u_t
\end{aligned}$$

**Definiendo nuevas variables:**

$$Z_0 = \sum_{i=0}^{n} \dot{M}_{t-i}$$
$$Z_1 = \sum_{i=0}^{n} i \cdot \dot{M}_{t-i}$$
$$Z_2 = \sum_{i=0}^{n} i^2 \cdot \dot{M}_{t-i}$$

**Modelo transformado:**

$$\dot{P}_t = \alpha + \gamma_0 Z_0 + \gamma_1 Z_1 + \gamma_2 Z_2 + u_t$$

**Ventaja:**

Solo 3 parámetros ($\gamma_0, \gamma_1, \gamma_2$) en lugar de $n+1$ parámetros ($m_0, m_1, \ldots, m_n$)

**Resultado empírico típico:**

$$\sum_{i=0}^{n} m_i \approx 1.03$$

Validando la **neutralidad del dinero** a largo plazo.

### 7.2 Función de Producción Cobb-Douglas

#### Modelo Original

$$Q = A K^{\beta_2} L^{\beta_3} e^u$$

**Forma log-lineal:**

$$\ln Q = \beta_1 + \beta_2 \ln K + \beta_3 \ln L + u$$

**Problema:**

$\ln K$ y $\ln L$ típicamente tienen correlación $> 0.90$ en datos de series de tiempo

#### Solución 1: Restricción de Rendimientos Constantes

**Si $\beta_2 + \beta_3 = 1$:**

$$\ln(Q/L) = \beta_1 + \beta_2 \ln(K/L) + u$$

**Interpretación:**

- Variable dependiente: **Productividad del trabajo** $(Q/L)$
- Variable independiente: **Intensidad de capital** $(K/L)$
- $\beta_2$: **Elasticidad de sustitución capital-trabajo**

#### Solución 2: Primeras Diferencias

$$\Delta \ln Q_t = \beta_2 \Delta \ln K_t + \beta_3 \Delta \ln L_t + \Delta u_t$$

**Interpretación:**

- $\beta_2$: Elasticidad del producto respecto al **crecimiento** del capital
- $\beta_3$: Elasticidad del producto respecto al **crecimiento** del trabajo

### 7.3 Demanda de Dinero

#### Modelo Tradicional

$$\ln(M/P) = \beta_1 + \beta_2 \ln Y + \beta_3 i + u$$

**Variables:**
- $M/P$: Saldos reales
- $Y$: Ingreso real
- $i$: Tasa de interés nominal

**Problema:**

En series de tiempo, $\ln Y$ e $i$ suelen estar correlacionados (ambos crecen en épocas de expansión)

#### Diagnóstico

```
Regresión auxiliar: ln Y = γ₁ + γ₂ i + v
R² = 0.78

FIV(ln Y) = 1/(1-0.78) = 4.55
FIV(i) = 1/(1-0.78) = 4.55

Diagnóstico: Multicolinealidad MODERADA
```

#### Solución: Diferenciación de Corto vs. Largo Plazo

**Modelo de Corrección de Error (ECM):**

$$\Delta \ln(M/P)_t = \beta_1 + \beta_2 \Delta \ln Y_t + \beta_3 \Delta i_t - \lambda[\ln(M/P)_{t-1} - \theta_1 \ln Y_{t-1} - \theta_2 i_{t-1}] + u_t$$

**Ventaja:**

Las primeras diferencias reducen multicolinealidad, mientras el término de corrección de error captura la relación de largo plazo.

---

## 8. Referencias y Lecturas

### 8.1 Bibliografía Fundamental

#### Textos Clásicos

1. **Gujarati, D. N. & Porter, D. C.** (2009). *Econometría* (5ª ed.). McGraw-Hill.
   - **Capítulo 10:** "Multicolinealidad: ¿Qué sucede si las variables explicativas están correlacionadas?"
   - Secciones clave:
     - 10.1-10.4: Naturaleza y consecuencias
     - 10.5: Detección práctica
     - 10.6: Medidas remediales
     - 10.7: ¿Actuar o no actuar?

2. **Wooldridge, J. M.** (2015). *Introducción a la Econometría* (4ª ed.). Cengage Learning.
   - **Capítulo 3:** Análisis de regresión múltiple: estimación
   - **Capítulo 4:** Análisis de regresión múltiple: inferencia
   - Enfoque en teorema de Gauss-Markov bajo multicolinealidad

3. **Greene, W. H.** (2018). *Econometric Analysis* (8th ed.). Pearson.
   - **Chapter 4.6:** Collinearity
   - **Chapter 20:** Models with lagged variables (Almon lag)

#### Textos Avanzados

4. **Davidson, R. & MacKinnon, J. G.** (2004). *Econometric Theory and Methods*. Oxford University Press.
   - Tratamiento formal de condición numérica y estabilidad computacional

5. **Hamilton, J. D.** (1994). *Time Series Analysis*. Princeton University Press.
   - Multicolinealidad en modelos de series temporales

### 8.2 Artículos Seminales

1. **Farrar, D. E. & Glauber, R. R.** (1967). "Multicollinearity in Regression Analysis: The Problem Revisited." *The Review of Economics and Statistics*, 49(1), 92-107.

2. **Belsley, D. A., Kuh, E., & Welsch, R. E.** (1980). *Regression Diagnostics: Identifying Influential Data and Sources of Collinearity*. Wiley.

3. **Hoerl, A. E. & Kennard, R. W.** (1970). "Ridge Regression: Biased Estimation for Nonorthogonal Problems." *Technometrics*, 12(1), 55-67.

### 8.3 Recursos Computacionales

#### Software Estadístico

**R:**
```r
# Calcular FIV
library(car)
vif(modelo)

# Regresión Ridge
library(glmnet)
ridge_modelo <- glmnet(x, y, alpha = 0)

# LASSO
lasso_modelo <- glmnet(x, y, alpha = 1)
```

**Python:**
```python
# FIV
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Ridge/LASSO
from sklearn.linear_model import Ridge, Lasso
```

**Stata:**
```stata
* FIV
estat vif

* Regresión con restricciones
constraint define 1 beta2 + beta3 = 1
cnsreg y x1 x2, constraints(1)
```

### 8.4 Tareas Recomendadas

#### Ejercicio 1: Cálculo Manual de FIV

**Datos:** Archivo `consumo.csv` (disponible en repositorio del curso)

**Variables:**
- $C$: Consumo agregado
- $Y$: Ingreso disponible
- $W$: Riqueza neta
- $i$: Tasa de interés

**Tarea:**

a) Estimar el modelo:
   $$C = \beta_1 + \beta_2 Y + \beta_3 W + \beta_4 i + u$$

b) Calcular manualmente:
   - Regresiones auxiliares
   - $R_j^2$ para cada variable
   - $\text{FIV}_j$ y $\text{TOL}_j$

c) Interpretar resultados y sugerir remedios

#### Ejercicio 2: Test de Farrar-Glauber

**Datos:** Mismos del Ejercicio 1

**Tarea:**

a) Construir la matriz de correlación $R_{XX}$
b) Calcular $|R_{XX}|$
c) Ejecutar el test $\chi^2$ de Bartlett
d) Interpretar al 5% de significancia

#### Ejercicio 3: Comparación de Remedios

**Datos:** Serie de tiempo de inflación y oferta monetaria (1960-2020)

**Tarea:**

a) Estimar modelo con rezagos de $\dot{M}$:
   $$\dot{P}_t = \alpha + \sum_{i=0}^{8} m_i \dot{M}_{t-i} + u_t$$

b) Diagnosticar multicolinealidad
c) Aplicar 3 remedios diferentes:
   - Eliminar algunos rezagos
   - Restricción Almon de grado 2
   - Ridge regression

d) Comparar resultados (coeficientes, errores estándar, $R^2$, interpretabilidad)

---

## 9. Comandos Útiles para Neovim/Gentoo

### 9.1 Alias para Compilación de LaTeX

Añadir al archivo `~/.bashrc`:

```bash
# Compilación de notas de econometría
alias texeco='pdflatex multicolinealidad.tex && \
              bibtex multicolinealidad && \
              pdflatex multicolinealidad.tex && \
              pdflatex multicolinealidad.tex'

# Visualización automática
alias vieweco='zathura multicolinealidad.pdf &'

# Compilar y ver en un solo comando
alias ecofull='texeco && vieweco'

# Limpiar archivos auxiliares
alias texclean='rm -f *.aux *.log *.out *.toc *.bbl *.blg'
```

### 9.2 Snippets de Neovim para LaTeX

Crear archivo `~/.config/nvim/UltiSnips/tex.snippets`:

```vim
snippet varmatrix "Variance-Covariance Matrix" b
\begin{bmatrix}
\text{Var}(\hat{\beta}_1) & \text{Cov}(\hat{\beta}_1, \hat{\beta}_2) & \text{Cov}(\hat{\beta}_1, \hat{\beta}_3) \\
\text{Cov}(\hat{\beta}_2, \hat{\beta}_1) & \text{Var}(\hat{\beta}_2) & \text{Cov}(\hat{\beta}_2, \hat{\beta}_3) \\
\text{Cov}(\hat{\beta}_3, \hat{\beta}_1) & \text{Cov}(\hat{\beta}_3, \hat{\beta}_2) & \text{Var}(\hat{\beta}_3)
\end{bmatrix}
endsnippet

snippet vif "VIF Formula" b
\text{FIV}_j = \frac{1}{1 - R_j^2}
endsnippet

snippet fgtest "Farrar-Glauber Test" b
\chi^2_{\text{calc}} = -\left[ n - 1 - \frac{1}{6}(2k + 5) \right] \ln |R_{XX}|
endsnippet

snippet regaux "Auxiliary Regression" b
X_${1:j} = \gamma_1 + \gamma_2 X_2 + \cdots + \gamma_k X_k + v_$1
endsnippet
```

### 9.3 Configuración de Markdown Preview en Neovim

Añadir a `~/.config/nvim/init.vim`:

```vim
" Markdown preview
Plug 'iamcco/markdown-preview.nvim', { 'do': 'cd app && yarn install' }

" Atajo para preview
nmap <leader>mp <Plug>MarkdownPreview
nmap <leader>ms <Plug>MarkdownPreviewStop

" Renderizado de matemáticas
let g:mkdp_preview_options = {
    \ 'mkit': {},
    \ 'katex': {},
    \ 'uml': {},
    \ 'maid': {},
    \ 'disable_sync_scroll': 0,
    \ 'sync_scroll_type': 'middle'
    \ }
```

### 9.4 Script para Conversión Markdown → PDF

Crear archivo `~/bin/md2pdf_eco`:

```bash
# !/bin/bash
# Convertir Markdown a PDF con soporte matemático

if [ -z "$1" ]; then
    echo "Uso: md2pdf_eco archivo.md"
    exit 1
fi

pandoc "$1" \
    --from markdown+tex_math_dollars \
    --to pdf \
    --output "${1%.md}.pdf" \
    --pdf-engine=xelatex \
    --variable geometry:margin=2.5cm \
    --variable fontsize=11pt \
    --variable mainfont="Latin Modern Roman" \
    --variable mathfont="Latin Modern Math" \
    --highlight-style=tango \
    --toc \
    --number-sections

echo "✓ PDF generado: ${1%.md}.pdf"
```

Hacer ejecutable:
```bash
chmod +x ~/bin/md2pdf_eco
```

Uso:
```bash
md2pdf_eco multicolinealidad.md
```

---

## 10. Apéndice: Tablas de Valores Críticos

### 10.1 Distribución $\chi^2$ (Chi-cuadrado)

**Para Test de Farrar-Glauber**

| gl | $\alpha = 0.10$ | $\alpha = 0.05$ | $\alpha = 0.01$ |
|----|-----------------|-----------------|-----------------|
| 1  | 2.706 | 3.841 | 6.635 |
| 2  | 4.605 | 5.991 | 9.210 |
| 3  | 6.251 | 7.815 | 11.345 |
| 4  | 7.779 | 9.488 | 13.277 |
| 5  | 9.236 | 11.070 | 15.086 |
| 6  | 10.645 | 12.592 | 16.812 |
| 10 | 15.987 | 18.307 | 23.209 |
| 15 | 22.307 | 24.996 | 30.578 |
| 20 | 28.412 | 31.410 | 37.566 |

**Fórmula de grados de libertad:**
$$gl = \frac{k(k-1)}{2}$$

### 10.2 Distribución $F$ (Fisher)

**Para Pruebas de Regresiones Auxiliares**

**Valores críticos $F_{0.05}$ (numerador, denominador)**

| $gl_1 \backslash gl_2$ | 30 | 40 | 60 | 120 | $\infty$ |
|------------------------|----|----|----|----|----------|
| 1 | 4.17 | 4.08 | 4.00 | 3.92 | 3.84 |
| 2 | 3.32 | 3.23 | 3.15 | 3.07 | 3.00 |
| 3 | 2.92 | 2.84 | 2.76 | 2.68 | 2.60 |
| 4 | 2.69 | 2.61 | 2.53 | 2.45 | 2.37 |
| 5 | 2.53 | 2.45 | 2.37 | 2.29 | 2.21 |
| 10 | 2.16 | 2.08 | 1.99 | 1.91 | 1.83 |

---

**Última actualización:** Viernes, 13 de febrero de 2026  
**Versión:** 1.0  
**Licencia:** Uso académico exclusivo - UPTC

---

```
████████╗██╗  ██╗███████╗     ███████╗███╗   ██╗██████╗
╚══██╔══╝██║  ██║██╔════╝     ██╔════╝████╗  ██║██╔══██╗
   ██║   ███████║█████╗       █████╗  ██╔██╗ ██║██║  ██║
   ██║   ██╔══██║██╔══╝       ██╔══╝  ██║╚██╗██║██║  ██║
   ██║   ██║  ██║███████╗     ███████╗██║ ╚████║██████╔╝
   ╚═╝   ╚═╝  ╚═╝╚══════╝     ╚══════╝╚═╝  ╚═══╝╚═════╝
```

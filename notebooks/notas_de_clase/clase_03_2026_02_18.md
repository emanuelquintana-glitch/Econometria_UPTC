# V. Evaluación de Bondad de Ajuste y Comparabilidad de Modelos en Econometría

> **Nota de estudio:** Este documento combina los apuntes de clase con teoría econométrica formal de alto rigor. Cada sección está diseñada para ser autocontenida pero conectada con las demás. Se asume familiaridad con álgebra matricial básica y estadística inferencial.

---

## Tabla de Contenidos

1. [Contexto: El Modelo de Cobb-Douglas y las Restricciones Paramétricas](#1-contexto)
2. [El Problema del R² No Comparable](#2-r2)
3. [Métodos de Comparación en Escala Común](#3-comparacion)
4. [La Prueba F de Wald: Contraste de Restricciones Lineales](#4-prueba-f)
5. [Propiedades Asintóticas y el Test de Razón de Verosimilitud](#5-asintotico)
6. [Criterios de Información para Selección de Modelos](#6-criterios)
7. [Diagnóstico Integral y Estrategia de Decisión](#7-diagnostico)
8. [Resumen Ejecutivo](#8-resumen)

---

## 1. Contexto: El Modelo de Cobb-Douglas y las Restricciones Paramétricas {#1-contexto}

### 1.1. El Modelo de Producción Cobb-Douglas

La función de producción Cobb-Douglas es uno de los instrumentos más utilizados en economía para modelar la relación entre insumos (capital $K$ y trabajo $L$) y producción $Y$:

$$Y = A \cdot K^{\beta_2} \cdot L^{\beta_3}$$

Donde:

- $A > 0$ es la **productividad total de los factores (PTF)**, también llamada parámetro de eficiencia tecnológica
- $\beta_2 > 0$ es la **elasticidad de la producción respecto al capital**
- $\beta_3 > 0$ es la **elasticidad de la producción respecto al trabajo**
- La suma $\beta_2 + \beta_3$ determina los **rendimientos a escala**

#### Propiedades de los rendimientos a escala

| Condición | Tipo de Rendimientos | Interpretación Económica |
|:---|:---:|:---|
| $\beta_2 + \beta_3 > 1$ | Crecientes (RCE+) | Doblar insumos más que dobla la producción |
| $\beta_2 + \beta_3 = 1$ | Constantes (RCE) | Doblar insumos exactamente dobla la producción |
| $\beta_2 + \beta_3 < 1$ | Decrecientes (RDE) | Doblar insumos menos que dobla la producción |

### 1.2. Linealización: De la Forma Multiplicativa a la Log-Lineal

Aplicando logaritmo natural a ambos lados de la función Cobb-Douglas:

$$\ln Y = \ln A + \beta_2 \ln K + \beta_3 \ln L$$

Definiendo $\beta_1 = \ln A$ y añadiendo un término de error $\varepsilon$, obtenemos el **Modelo No Restringido (MNR)**:

$$\underbrace{\ln Y_i}_{\text{Variable dependiente}} = \underbrace{\beta_1}_{\text{intercepto}} + \underbrace{\beta_2}_{\text{elasticidad K}} \ln K_i + \underbrace{\beta_3}_{\text{elasticidad L}} \ln L_i + \varepsilon_i$$

> **Intuición de los parámetros:** En el modelo log-lineal, $\beta_2$ representa el porcentaje de aumento en $Y$ ante un aumento de 1% en $K$, *ceteris paribus*. Esta es la definición exacta de elasticidad, lo que hace al modelo Cobb-Douglas especialmente conveniente para el análisis económico.

### 1.3. La Restricción de Rendimientos Constantes a Escala (RCE)

La hipótesis de RCE implica:

$$H_0: \beta_2 + \beta_3 = 1 \quad \Longleftrightarrow \quad \beta_3 = 1 - \beta_2$$

Sustituyendo en el modelo:

$$\ln Y_i = \beta_1 + \beta_2 \ln K_i + (1 - \beta_2) \ln L_i + \varepsilon_i$$

Reagrupando algebraicamente:

$$\ln Y_i - \ln L_i = \beta_1 + \beta_2 (\ln K_i - \ln L_i) + \varepsilon_i$$

Usando propiedades de logaritmos ($\ln a - \ln b = \ln(a/b)$):

$$\underbrace{\ln\left(\frac{Y_i}{L_i}\right)}_{\text{Log-productividad}} = \beta_1 + \beta_2 \underbrace{\ln\left(\frac{K_i}{L_i}\right)}_{\text{Log-intensidad capital}} + \varepsilon_i$$

Este es el **Modelo Restringido (MR)**: una regresión simple de log-productividad sobre log-intensidad de capital.

### 1.4. El Problema de la Multicolinealidad y Por Qué Imponemos Restricciones

En la práctica econométrica, cuando estimamos el MNR con datos de series de tiempo, es común encontrar **multicolinealidad severa** entre $\ln K$ y $\ln L$. Esto ocurre porque:

1. El capital y el trabajo tienden a crecer conjuntamente en una economía en expansión
2. Ambas series tienen tendencias temporales similares
3. Las decisiones de contratación e inversión están correlacionadas

Las consecuencias de la multicolinealidad severa son:

- **Varianzas infladas** de los estimadores $\hat{\beta}_2$ y $\hat{\beta}_3$
- **Signos incorrectos** en los coeficientes (incluso con resultados económicamente absurdos como $\hat{\beta}_2 < 0$)
- **Intervalos de confianza muy amplios** que dificultan la inferencia
- **$R^2$ alto** pero **estadísticos $t$ bajos** (una firma señal diagnóstica)

> **Solución:** Si la teoría económica (respaldada por la prueba F) no rechaza los RCE, podemos imponer $\beta_2 + \beta_3 = 1$ y estimar el MR. Al hacer esto, eliminamos un parámetro libre, reducimos la dimensión del problema y corregimos los signos inducidos por la multicolinealidad.

---

## 2. El Problema del $R^2$ No Comparable {#2-r2}

### 2.1. Definición Formal del $R^2$

El coeficiente de determinación se define como:

$$R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}$$

Donde:

- $TSS = \sum_{i=1}^{n}(y_i - \bar{y})^2$ es la **Suma Total de Cuadrados** (varianza total de la variable dependiente)
- $RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}\hat{\varepsilon}_i^2$ es la **Suma de Cuadrados de Residuos**
- $ESS = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$ es la **Suma de Cuadrados Explicada**
- Y se cumple la identidad: $TSS = ESS + RSS$

El $R^2$ varía entre 0 y 1 **sólo cuando el modelo incluye un intercepto** y mide la fracción de varianza de $y$ explicada por las variables regresoras.

### 2.2. Por Qué el $R^2$ No Es Comparable Entre Modelos con Diferente Variable Dependiente

**Modelo No Restringido:** Variable dependiente = $\ln Y$

$$R^2_{MNR} = 1 - \frac{\sum(\ln Y_i - \widehat{\ln Y}_i)^2}{\sum(\ln Y_i - \overline{\ln Y})^2} = 1 - \frac{RSS_{MNR}}{TSS_{\ln Y}}$$

**Modelo Restringido:** Variable dependiente = $\ln(Y/L)$

$$R^2_{MR} = 1 - \frac{\sum\left[\ln\left(\frac{Y_i}{L_i}\right) - \widehat{\ln\left(\frac{Y}{L}\right)}_i\right]^2}{\sum\left[\ln\left(\frac{Y_i}{L_i}\right) - \overline{\ln\left(\frac{Y}{L}\right)}\right]^2} = 1 - \frac{RSS_{MR}}{TSS_{\ln(Y/L)}}$$

El problema es que los denominadores son **fundamentalmente distintos**:

$$TSS_{\ln Y} = \sum(\ln Y_i - \overline{\ln Y})^2 \neq TSS_{\ln(Y/L)} = \sum\left[\ln\left(\frac{Y_i}{L_i}\right) - \overline{\ln\left(\frac{Y}{L}\right)}\right]^2$$

#### Demostración de la no-comparabilidad

Nótese que $\ln(Y/L) = \ln Y - \ln L$. Por lo tanto:

$$TSS_{\ln(Y/L)} = \text{Var}(\ln Y - \ln L) \cdot (n-1) = \left[\text{Var}(\ln Y) + \text{Var}(\ln L) - 2\text{Cov}(\ln Y, \ln L)\right](n-1)$$

Si $\ln K$ y $\ln L$ están fuertemente correlacionados (caso típico de multicolinealidad), entonces $\text{Var}(\ln(Y/L)) \ll \text{Var}(\ln Y)$, lo que **comprime el $TSS$** del MR artificialmente.

> **Consecuencia práctica:** El MR puede tener un $R^2$ de, digamos, $0.65$ mientras el MNR tiene $0.97$. Sería un error concluir que el MNR "explica mejor" la realidad, porque los $R^2$ están midiendo fracciones de varianzas completamente distintas. Es como comparar qué porcentaje de una pizza come una persona versus qué porcentaje de un pastel come otra: el denominador es diferente.

---

## 3. Métodos de Comparación en Escala Común {#3-comparacion}

Para comparar "peras con peras", necesitamos llevar ambos modelos a **la misma unidad de medida**. Existen dos métodos canónicos.

### 3.1. Método 1: Transformación del MR a la Escala de $\ln Y$

**Paso 1:** Estimar el MR por MCO y obtener los valores ajustados:

$$\widehat{\ln\left(\frac{Y}{L}\right)}_i = \hat{\beta}_1 + \hat{\beta}_2 \ln\left(\frac{K_i}{L_i}\right)$$

**Paso 2:** Reconstruir la predicción de $\ln Y$ sumando $\ln L_i$ a ambos lados:

$$\widehat{\ln Y}_i^{MR} = \widehat{\ln\left(\frac{Y}{L}\right)}_i + \ln L_i$$

**Paso 3:** Calcular el coeficiente de correlación de Pearson entre los valores reales y los valores predichos reconstruidos:

$$r = \frac{\sum\left(\ln Y_i - \overline{\ln Y}\right)\left(\widehat{\ln Y}_i^{MR} - \overline{\widehat{\ln Y}^{MR}}\right)}{\sqrt{\sum\left(\ln Y_i - \overline{\ln Y}\right)^2 \cdot \sum\left(\widehat{\ln Y}_i^{MR} - \overline{\widehat{\ln Y}^{MR}}\right)^2}}$$

**Paso 4:** El estadístico de comparación es $r^2$, que ahora es directamente comparable con $R^2_{MNR}$.

#### ¿Por qué $r^2$ y no $R^2$?

En el MNR, el $R^2$ coincide con el cuadrado del coeficiente de correlación entre $\ln Y_i$ y $\widehat{\ln Y}_i$:

$$R^2_{MNR} = r^2\left(\ln Y, \widehat{\ln Y}^{MNR}\right)$$

Al usar también $r^2$ para el MR (pero comparando $\ln Y$ real con $\widehat{\ln Y}^{MR}$), mantenemos la misma métrica de comparación en ambos casos.

> **Advertencia técnica:** La reconstrucción $\widehat{\ln Y}_i^{MR} = \widehat{\ln(Y/L)}_i + \ln L_i$ es exacta algebraicamente, pero el $r^2$ resultante **no** es el $R^2$ de una regresión de $\ln Y$ sobre las variables del MR. Los residuos de esta comparación no son los mismos que los residuos de la regresión original.

### 3.2. Método 2: Comparación en Niveles Originales (Escala de $Y$)

Este método es más informativo porque evalúa el ajuste en la escala económicamente interpretable (producción en niveles, no logaritmos).

**Paso 1:** Aplicar la transformación exponencial (antilogaritmo) a las predicciones de cada modelo:

$$\hat{Y}_i^{MNR} = e^{\widehat{\ln Y}_i^{MNR}} = e^{\hat{\beta}_1 + \hat{\beta}_2 \ln K_i + \hat{\beta}_3 \ln L_i}$$

$$\hat{Y}_i^{MR} = e^{\widehat{\ln Y}_i^{MR}} = e^{\hat{\beta}_1 + \hat{\beta}_2 \ln(K_i/L_i) + \ln L_i}$$

**Paso 2:** Calcular la correlación entre $Y_i$ real y $\hat{Y}_i$ predicho para cada modelo:

$$r^2_{niveles,\, MNR} = \left[r\left(Y_i,\, \hat{Y}_i^{MNR}\right)\right]^2$$

$$r^2_{niveles,\, MR} = \left[r\left(Y_i,\, \hat{Y}_i^{MR}\right)\right]^2$$

**Paso 3:** El modelo con mayor $r^2$ en niveles se declara superior en términos de ajuste.

#### Corrección de Duan (Sesgo de Retransformación)

Existe un **sesgo de retransformación** al pasar de $\hat{\ln Y}$ a $\hat{Y}$ mediante la exponencial. Si $\varepsilon \sim N(0, \sigma^2)$, entonces:

$$E[Y | X] = E[e^{\ln Y} | X] = e^{E[\ln Y | X]} \cdot e^{\sigma^2/2} = e^{X'\beta} \cdot e^{\sigma^2/2}$$

El término $e^{\sigma^2/2}$ es el **factor de corrección de sesgo**. Para comparaciones en niveles más rigurosas, usar:

$$\hat{Y}_i^{corr} = e^{\widehat{\ln Y}_i} \cdot e^{\hat{\sigma}^2/2}$$

donde $\hat{\sigma}^2 = RSS/(n-k)$ es el estimador insesgado de la varianza del error.

> **En la práctica:** Para comparar dos modelos entre sí (no para predicción puntual), el factor de corrección es el mismo para ambos modelos si tienen similar $\hat{\sigma}^2$, por lo que no altera el ranking. Sin embargo, para predicciones absolutas, omitirlo genera subestimación sistemática de $Y$.

---

## 4. La Prueba F de Wald: Contraste de Restricciones Lineales {#4-prueba-f}

### 4.1. Formulación General del Test de Wald

Consideremos el modelo general de regresión lineal múltiple:

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_n)$$

Donde $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_k)'$ es el vector de $k$ parámetros. Queremos contrastar la hipótesis:

$$H_0: \mathbf{R}\boldsymbol{\beta} = \mathbf{r}$$

Donde $\mathbf{R}$ es una matriz $(m \times k)$ de restricciones y $\mathbf{r}$ es un vector $(m \times 1)$ de constantes.

**Para nuestra restricción de RCE:**

- $m = 1$ restricción
- $\mathbf{R} = [0, 1, 1]$ (coeficientes de $\beta_1, \beta_2, \beta_3$)
- $\mathbf{r} = [1]$
- La hipótesis es: $0 \cdot \beta_1 + 1 \cdot \beta_2 + 1 \cdot \beta_3 = 1$, es decir, $\beta_2 + \beta_3 = 1$

### 4.2. La Estadística $F$ Basada en $RSS$

La forma más intuitiva de la prueba F compara las sumas de cuadrados de residuos:

$$\boxed{F = \frac{(RSS_R - RSS_{UR}) / m}{RSS_{UR} / (n - k)} \sim F(m,\, n-k) \text{ bajo } H_0}$$

Donde:

| Símbolo | Definición | Propiedad |
|:---:|:---|:---|
| $RSS_R$ | Suma de cuadrados de residuos del **Modelo Restringido** | Siempre $\geq RSS_{UR}$ |
| $RSS_{UR}$ | Suma de cuadrados de residuos del **Modelo No Restringido** | Es el mínimo teórico |
| $m$ | Número de restricciones lineales independientes | $m = 1$ en nuestro caso |
| $n$ | Número de observaciones | Tamaño muestral |
| $k$ | Número de parámetros en el MNR (incluyendo intercepto) | $k = 3$ aquí: $\beta_1, \beta_2, \beta_3$ |
| $n - k$ | Grados de libertad del MNR | Capacidad de maniobra estadística |

#### ¿Por qué $RSS_R \geq RSS_{UR}$ siempre?

El MNR minimiza $RSS$ sin restricciones sobre $\boldsymbol{\beta}$. Al imponer $\beta_2 + \beta_3 = 1$, restringimos el espacio de búsqueda. Un mínimo sobre un conjunto restringido **nunca puede ser menor** que el mínimo sobre el conjunto sin restricciones. Formalmente:

$$\boldsymbol{\hat{\beta}}_{UR} = \arg\min_{\boldsymbol{\beta}} \sum \varepsilon_i^2 \implies RSS_{UR} \leq RSS_R$$

La diferencia $RSS_R - RSS_{UR} \geq 0$ mide el **costo en ajuste** de imponer la restricción.

### 4.3. Interpretación Económica del Numerador y Denominador

**Numerador:** $(RSS_R - RSS_{UR}) / m$

Esta es la **pérdida de ajuste promedio por restricción impuesta**. Si la restricción es verdadera ($H_0$ cierta), esta pérdida debería ser pequeña (ruido estadístico). Si la restricción es falsa, la pérdida será grande.

**Denominador:** $RSS_{UR} / (n-k)$

Este es $\hat{\sigma}^2_{UR}$, el **estimador insesgado de la varianza del error** en el MNR. Sirve como "escala" natural para medir qué tan grande es la pérdida en el numerador.

> **Intuición:** La estadística $F$ pregunta: "¿La pérdida de ajuste al imponer la restricción ($RSS_R - RSS_{UR}$) es grande en relación al error propio del modelo ($\hat{\sigma}^2_{UR}$)?" Si la respuesta es no, la restricción es compatible con los datos.

### 4.4. Equivalencia con la Forma Cuadrática de Wald

Algebraicamente, la estadística $F$ es equivalente a:

$$F = \frac{(\mathbf{R}\hat{\boldsymbol{\beta}}_{UR} - \mathbf{r})'\left[\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'\right]^{-1}(\mathbf{R}\hat{\boldsymbol{\beta}}_{UR} - \mathbf{r})}{m \cdot \hat{\sigma}^2}$$

Para nuestra restricción $\beta_2 + \beta_3 = 1$:

- $\mathbf{R}\hat{\boldsymbol{\beta}}_{UR} - \mathbf{r} = \hat{\beta}_2 + \hat{\beta}_3 - 1$ (cuánto se desvía la estimación de la hipótesis nula)
- El término matricial $\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'$ es escalar en este caso: $\text{Var}(\hat{\beta}_2 + \hat{\beta}_3) = \text{Var}(\hat{\beta}_2) + \text{Var}(\hat{\beta}_3) + 2\text{Cov}(\hat{\beta}_2, \hat{\beta}_3)$

Esto simplifica a:

$$F = \frac{(\hat{\beta}_2 + \hat{\beta}_3 - 1)^2}{\widehat{\text{Var}}(\hat{\beta}_2 + \hat{\beta}_3)}$$

Y como este es un cociente cuadrático normalizado con 1 g.l. en el numerador, tenemos la relación:

$$F(1, n-k) = \left[t(n-k)\right]^2$$

Es decir, para restricciones únicas ($m = 1$), la prueba $F$ y la prueba $t$ son exactamente equivalentes.

### 4.5. Grados de Libertad: Derivación Formal

**Numerador ($m$ grados de libertad):**

Bajo $H_0$ y los supuestos CLM, el estadístico:

$$\frac{RSS_R - RSS_{UR}}{\sigma^2} \sim \chi^2(m)$$

Los $m$ grados de libertad representan la dimensión del subespacio de restricciones. Cada restricción lineal independiente "consume" un grado de libertad.

**Denominador ($n - k$ grados de libertad):**

Bajo los supuestos CLM:

$$\frac{RSS_{UR}}{\sigma^2} \sim \chi^2(n-k)$$

Los $n$ grados de libertad totales de la muestra se "consumen": $k$ por estimar los $k$ parámetros del modelo, dejando $n - k$ residuales.

**La distribución $F$** es precisamente el cociente de dos $\chi^2$ independientes divididos por sus respectivos grados de libertad:

$$F = \frac{\chi^2(m)/m}{\chi^2(n-k)/(n-k)} \sim F(m,\, n-k)$$

La independencia entre numerador y denominador se garantiza por el teorema de Cochran (bajo normalidad de los errores).

### 4.6. Regla de Decisión y Nivel de Significancia

#### Decisión basada en valor crítico

$$\text{Si } F_{calc} > F_{\alpha}(m, n-k) \Rightarrow \text{Rechazar } H_0$$

$$\text{Si } F_{calc} \leq F_{\alpha}(m, n-k) \Rightarrow \text{No rechazar } H_0$$

donde $F_{\alpha}(m, n-k)$ es el cuantil $1-\alpha$ de la distribución $F$ con $m$ y $n-k$ grados de libertad.

#### Decisión basada en p-valor

$$p\text{-valor} = P\left[F(m, n-k) > F_{calc}\right]$$

$$\text{Si } p < \alpha \Rightarrow \text{Rechazar } H_0 \quad ; \quad \text{Si } p \geq \alpha \Rightarrow \text{No rechazar } H_0$$

#### Implicaciones para la selección de modelo

| Resultado | Interpretación | Modelo Preferido |
|:---:|:---|:---:|
| $F_{calc} > F_{\alpha}$ (rechazamos $H_0$) | Los datos rechazan los RCE. La restricción es demasiado costosa en términos de ajuste. | **MNR** (a pesar de la multicolinealidad) |
| $F_{calc} \leq F_{\alpha}$ (no rechazamos $H_0$) | Los datos son compatibles con los RCE. La restricción es "gratuita" estadísticamente. | **MR** (más parsimonioso, signos correctos) |

> **Cuidado metodológico:** "No rechazar $H_0$" **no es lo mismo** que "aceptar $H_0$". La prueba puede carecer de potencia estadística (tamaño muestral pequeño) para detectar desviaciones moderadas de los RCE. Siempre reportar el $p$-valor, no solo la decisión binaria.

---

## 5. Propiedades Asintóticas y el Test de Razón de Verosimilitud {#5-asintotico}

### 5.1. Relación entre la Prueba F y la Razón de Verosimilitud

Para el modelo de regresión lineal normal, la prueba $F$ es **equivalente exacta** al Test de Razón de Verosimilitud (LRT). El estadístico LRT es:

$$LR = -2\ln\left(\frac{\mathcal{L}_{MR}}{\mathcal{L}_{MNR}}\right) = n \cdot \ln\left(\frac{RSS_R}{RSS_{UR}}\right)$$

Bajo $H_0$, $LR \xrightarrow{d} \chi^2(m)$ asintóticamente. La relación exacta (en muestras finitas con normalidad) es:

$$F = \frac{e^{LR/n} - 1}{m/(n-k)}$$

### 5.2. Potencia de la Prueba

La **potencia** es la probabilidad de rechazar $H_0$ cuando es falsa ($1 - \beta$, donde $\beta$ es la probabilidad de error tipo II).

Para la prueba $F$, bajo $H_1: \mathbf{R}\boldsymbol{\beta} \neq \mathbf{r}$, la estadística $F$ sigue una distribución **$F$ no central**:

$$F \sim F(m, n-k, \lambda)$$

donde el **parámetro de no-centralidad** es:

$$\lambda = \frac{(\mathbf{R}\boldsymbol{\beta} - \mathbf{r})'\left[\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'\right]^{-1}(\mathbf{R}\boldsymbol{\beta} - \mathbf{r})}{\sigma^2}$$

La potencia aumenta con:

1. **Mayor $\lambda$:** Restricción más alejada de la verdad ($|\mathbf{R}\boldsymbol{\beta} - \mathbf{r}|$ grande)
2. **Mayor $n$:** Más información muestral
3. **Menor $\sigma^2$:** Datos menos ruidosos

> **Implicación práctica:** Con muestras pequeñas ($n < 30$), una prueba $F$ que "no rechaza $H_0$" puede simplemente carecer de potencia suficiente, no necesariamente confirmar los RCE.

---

## 6. Criterios de Información para Selección de Modelos {#6-criterios}

### 6.1. El Problema del $R^2$ Ajustado

El $R^2$ nunca disminuye al agregar variables, incluso irrelevantes. Para corregir esto se usa el $R^2$ ajustado:

$$\bar{R}^2 = 1 - \frac{RSS/(n-k)}{TSS/(n-1)} = 1 - (1-R^2)\frac{n-1}{n-k}$$

**Puede disminuir** al agregar variables irrelevantes, penalizando la complejidad del modelo. Sin embargo, sigue siendo **no comparable** entre modelos con diferente variable dependiente.

### 6.2. Criterio de Información de Akaike (AIC)

$$AIC = n \cdot \ln\left(\frac{RSS}{n}\right) + 2k$$

- El primer término mide el ajuste (menor $RSS$ es mejor)
- El segundo término penaliza el número de parámetros ($k$)
- **Se prefiere el modelo con menor AIC**
- El AIC penaliza la complejidad pero puede sobreajustar en muestras grandes

Para modelos con diferente variable dependiente, el AIC **tampoco es directamente comparable** sin la corrección de transformación (similar al problema del $R^2$).

### 6.3. Criterio de Información Bayesiano (BIC/SIC)

$$BIC = n \cdot \ln\left(\frac{RSS}{n}\right) + k \cdot \ln(n)$$

- Penaliza los parámetros más severamente que el AIC cuando $n > e^2 \approx 7.4$
- **Se prefiere el modelo con menor BIC**
- El BIC tiende a seleccionar modelos más parsimoniosos que el AIC
- Es consistente (selecciona el modelo verdadero con probabilidad 1 cuando $n \to \infty$)

### 6.4. Comparación entre AIC y BIC

| Criterio | Penalización por parámetro | Propiedad | Preferencia |
|:---:|:---:|:---:|:---:|
| AIC | $2$ (constante) | Eficiente en predicción | Modelos más complejos |
| BIC | $\ln(n)$ (crece con $n$) | Consistente | Modelos más parsimoniosos |

> **Recomendación:** En econometría estructural (donde la teoría especifica el modelo verdadero, como en Cobb-Douglas), el BIC es preferible por su consistencia. En modelos predictivos sin teoría fuerte, el AIC es más relevante.

---

## 7. Diagnóstico Integral y Estrategia de Decisión {#7-diagnostico}

### 7.1. Árbol de Decisión para Selección de Modelo

```
¿El modelo presenta multicolinealidad severa?
│
├── NO → Estimar MNR, verificar signos, usar R² estándar
│
└── SÍ → ¿Los signos de los coeficientes son correctos?
         │
         ├── SÍ → Mantener MNR (multicolinealidad presente pero no perjudicial)
         │
         └── NO → Realizar prueba F de Wald (H₀: β₂ + β₃ = 1)
                  │
                  ├── p-valor < α (rechazar H₀) → MNR es preferido
                  │   (documentar el problema de signos como limitación)
                  │
                  └── p-valor ≥ α (no rechazar H₀) → Imponer RCE
                      Estimar MR: ln(Y/L) = β₁ + β₂ ln(K/L) + ε
                      Verificar: signos correctos, r² comparable favorable
```

### 7.2. Diagnóstico de Multicolinealidad: El Factor de Inflación de Varianza (FIV)

El FIV para la variable $X_j$ se define como:

$$FIV_j = \frac{1}{1 - R_j^2}$$

donde $R_j^2$ es el $R^2$ de la regresión de $X_j$ sobre todas las demás variables regresoras.

| Valor de $FIV_j$ | Interpretación |
|:---:|:---|
| $1$ | Sin multicolinealidad |
| $1 < FIV_j < 5$ | Multicolinealidad moderada, generalmente tolerable |
| $5 \leq FIV_j < 10$ | Multicolinealidad severa, considerar medidas correctivas |
| $FIV_j \geq 10$ | Multicolinealidad crítica, los estimadores son poco fiables |

### 7.3. Validación de Supuestos del MR

Después de estimar el MR, es esencial verificar que los supuestos de Gauss-Markov se mantienen:

**Homocedasticidad:** Los errores deben tener varianza constante.

- Test de Breusch-Pagan: $H_0: \sigma^2_i = \sigma^2$ (homocedasticidad)
- Test de White: más robusto, no asume forma funcional específica

**Autocorrelación (en series de tiempo):** Los errores no deben estar correlacionados entre sí.

- Estadístico de Durbin-Watson: $d = \frac{\sum_{t=2}^{n}(\hat{\varepsilon}_t - \hat{\varepsilon}_{t-1})^2}{\sum_{t=1}^{n}\hat{\varepsilon}_t^2}$
  - $d \approx 2$: No autocorrelación
  - $d < d_L$: Autocorrelación positiva
  - $d > 4 - d_L$: Autocorrelación negativa

**Normalidad de residuos:**

- Test de Jarque-Bera: $JB = \frac{n}{6}\left[S^2 + \frac{(K-3)^2}{4}\right] \sim \chi^2(2)$ bajo $H_0: \text{normalidad}$
  donde $S$ es el coeficiente de asimetría y $K$ es la curtosis

### 7.4. Análisis de Signos de Coeficientes

La restricción de RCE corrige los signos cuando la multicolinealidad los distorsiona. La validación económica requiere:

| Parámetro | Signo Esperado | Fundamento Económico |
|:---:|:---:|:---|
| $\hat{\beta}_1$ | Cualquier signo | Es $\ln A$; $A > 0$ pero $\ln A$ puede ser negativo si $A < 1$ |
| $\hat{\beta}_2$ | **Positivo** ($> 0$) | Elasticidad capital: más K siempre aumenta Y en Cobb-Douglas |
| $\hat{\beta}_3$ (solo en MNR) | **Positivo** ($> 0$) | Elasticidad trabajo: más L siempre aumenta Y en Cobb-Douglas |

Si en el MR el coeficiente $\hat{\beta}_2$ es positivo, el coeficiente implícito para $\ln L$ es $1 - \hat{\beta}_2$. Para que también sea positivo, necesitamos $\hat{\beta}_2 < 1$.

---

## 8. Resumen Ejecutivo {#8-resumen}

### 8.1. Flujo Completo de Estimación y Evaluación

```
Paso 1: Estimar MNR por MCO
         ln Y = β₁ + β₂ ln K + β₃ ln L + ε
         → Obtener β̂₁, β̂₂, β̂₃, R²_MNR, RSS_MNR

Paso 2: Diagnosticar multicolinealidad
         → Calcular FIV para ln K y ln L
         → Verificar signos de β̂₂ y β̂₃

Paso 3: Prueba F de Wald (H₀: β₂ + β₃ = 1)
         → Estimar MR: ln(Y/L) = β₁ + β₂ ln(K/L) + ε
         → Obtener RSS_MR
         → Calcular F = [(RSS_MR - RSS_MNR)/1] / [RSS_MNR/(n-3)]
         → Comparar con F_α(1, n-3)

Paso 4: Si no se rechaza H₀ → Usar MR
         → Calcular r² comparable (transformación a escala ln Y)
         → Calcular r² en niveles originales
         → Verificar signos correctos en MR

Paso 5: Reportar
         → F calculado y p-valor
         → R² comparables en escala común
         → Signos y magnitudes de coeficientes
         → Tests de especificación (heterocedasticidad, autocorrelación)
```

### 8.2. Tabla de Herramientas Diagnósticas

| Herramienta | Función Principal | Fórmula Clave | Criterio de Decisión |
|:---|:---|:---:|:---|
| **$R^2$ Ajustado** | Ajuste penalizado por complejidad | $1 - \frac{RSS/(n-k)}{TSS/(n-1)}$ | Solo comparable entre modelos con misma $y$ |
| **$r^2$ en escala $\ln Y$** | Comparar ajuste entre MNR y MR | $[r(\ln Y, \widehat{\ln Y}^{MR})]^2$ | Mayor $r^2$ indica mejor ajuste |
| **$r^2$ en niveles** | Ajuste en escala económica | $[r(Y, e^{\widehat{\ln Y}})]^2$ | Mayor $r^2$ indica mejor predicción |
| **Prueba F** | Validez estadística de la restricción | $F = \frac{(RSS_R - RSS_{UR})/m}{RSS_{UR}/(n-k)}$ | No rechazar $H_0$ valida RCE |
| **FIV** | Diagnóstico de multicolinealidad | $1/(1-R_j^2)$ | $FIV > 10$: problema severo |
| **Durbin-Watson** | Autocorrelación en residuos | $d \approx 2$ sin autocorrelación | $d < d_L$: autocorrelación positiva |
| **Signos de $\hat{\beta}$** | Consistencia económica | $\hat{\beta}_2 > 0$, $\hat{\beta}_3 > 0$ | Signos negativos: modelo problemático |

### 8.3. Síntesis Conceptual

El problema central de este módulo es la tensión entre dos objetivos a veces contrapuestos:

1. **Ajuste estadístico:** El MNR tiene más parámetros libres y por construcción $RSS_{MNR} \leq RSS_{MR}$. Siempre ajusta mejor o igual.

2. **Corrección teórica y parsimonia:** El MR impone una restricción económicamente motivada (RCE), reduce la dimensión del problema, corrige los efectos distorsionadores de la multicolinealidad y produce estimadores más eficientes si la restricción es cierta.

La prueba $F$ de Wald actúa como árbitro: **mide si el costo estadístico** (en términos de pérdida de ajuste, $RSS_R - RSS_{UR}$) **de imponer la restricción es estadísticamente significativo**. Si no lo es, la ganancia en interpretabilidad y consistencia teórica del MR supera el pequeño costo en ajuste, y preferimos el modelo restringido.

> **Principio de parsimonia (Navaja de Occam en econometría):** Entre dos modelos con poder explicativo similar (medido en escala comparable), siempre preferir el más simple. La parsimonia reduce el riesgo de sobreajuste (*overfitting*) y mejora la capacidad predictiva fuera de muestra.

---

*Apuntes de Econometría — Sección V: Evaluación de Bondad de Ajuste y Comparabilidad*  
*Complementados con teoría formal de Gujarati & Porter, Greene (Econometric Analysis), y Wooldridge (Introductory Econometrics)*
